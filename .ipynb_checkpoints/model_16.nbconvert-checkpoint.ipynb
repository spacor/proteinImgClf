{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, models\n",
    "\n",
    "from skimage import io\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches, patheffects\n",
    "\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# base_path = r'../input'\n",
    "base_path = r'input'\n",
    "PATH_TRAIN_ANNO = os.path.join(base_path, 'train.csv')\n",
    "PATH_TRAIN_IMG = os.path.join(base_path, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "583ead53ccf420ab4290230a0a17cc3b6c62c74d"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from tensorboardX import SummaryWriter\n",
    "    USE_TENSORBOARD = True\n",
    "    writer = SummaryWriter()\n",
    "except:\n",
    "    USE_TENSORBOARD = False\n",
    "    print('No tensorboard X')\n",
    "\n",
    "def record_tb(phase, tag, value, global_step):\n",
    "    if USE_TENSORBOARD is True:\n",
    "        writer.add_scalar('{phase}/{tag}'.format(phase=phase, tag=tag), value, global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "7a95c899247b666f9235e3100a569744c46bf943"
   },
   "outputs": [],
   "source": [
    "# os.listdir(PATH_TRAIN_IMG)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "e85715fa2a9217474fc039c2b399c9b33490689a"
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 28\n",
    "MAX_TAGS = 5\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "VAL_SIZE =0.2\n",
    "THRESHOLD = 0.5\n",
    "SAMPLES = 1\n",
    "# DEVICE = torch.device(\"cpu\")\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_WORKERS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "7e81577b2c725960293a5cfabb29e0d46d66834c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample size: 31072\n",
      "[('00070df0-bbc3-11e8-b2bc-ac1f6b6435d0', [16, 0]),\n",
      " ('000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0', [7, 1, 2, 0]),\n",
      " ('000a9596-bbc4-11e8-b2bc-ac1f6b6435d0', [5])]\n"
     ]
    }
   ],
   "source": [
    "def get_transform_anno(annotation_path, img_path):\n",
    "    df = pd.read_csv(annotation_path)\n",
    "    annotations = []\n",
    "    for i, row in df.iterrows():\n",
    "        rcd_id = row['Id']\n",
    "        rcd_cate =  [int(j) for j in row['Target'].split()]\n",
    "        annotations.append((rcd_id, rcd_cate))\n",
    "    return annotations\n",
    "#get annotations\n",
    "annotations = get_transform_anno(PATH_TRAIN_ANNO, PATH_TRAIN_IMG)\n",
    "sample_size = int(len(annotations) * SAMPLES)\n",
    "print('sample size: {}'.format(sample_size))\n",
    "annotations = annotations[:sample_size]\n",
    "pprint(annotations[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "878d1a4d5d29884cc313258347df1ee630abae1a"
   },
   "outputs": [],
   "source": [
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, img_meta, img_path, transform = None):\n",
    "        self.img_meta = img_meta\n",
    "        self.transform = transform\n",
    "        self.channels = ['red', 'blue', 'yellow', 'green']\n",
    "        self.img_path = img_path\n",
    "        self.mlb = MultiLabelBinarizer(classes=range(0,NUM_CLASSES))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_meta)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id, img_tags= self.img_meta[idx]\n",
    "        ch = []\n",
    "        img_file_template = '{}_{}.png'\n",
    "        for c in self.channels:\n",
    "            ch.append(io.imread(os.path.join(self.img_path, img_file_template.format(img_id, c))))\n",
    "        img = np.stack(ch)\n",
    "\n",
    "        #augmentation\n",
    "        if bool(self.transform) is True:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        #binarize\n",
    "        img_tags = self.mlb.fit_transform([img_tags]).squeeze()\n",
    "        \n",
    "        #transform to tensor\n",
    "        img = torch.from_numpy(img).float()\n",
    "        img_tags = torch.from_numpy(img_tags)\n",
    "        \n",
    "        output = (img, img_tags)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "af583f0882e55fd31a3df230822ac86f99e25f4c"
   },
   "outputs": [],
   "source": [
    "class ImgTfm:\n",
    "    def __init__(self, aug_pipline = None):\n",
    "        self.seq = aug_pipline\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        \n",
    "#         seq_det = self.seq.to_deterministic()\n",
    "        \n",
    "        #augmentation\n",
    "        aug_img=img.copy().transpose((1, 2, 0))\n",
    "        aug_img = self.seq.augment_images([aug_img])[0]\n",
    "        aug_img=aug_img.transpose((2, 1, 0))\n",
    "        \n",
    "        #normalize\n",
    "        aug_img=aug_img/255\n",
    "        \n",
    "        return aug_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "7c0aeb64e2a7b651582a467d82079a2fda67c71b"
   },
   "outputs": [],
   "source": [
    "def get_aug_pipline(img_size, mode = 'train'):\n",
    "    if mode == 'train':\n",
    "        seq = iaa.Sequential([\n",
    "            iaa.Scale({\"height\": IMG_SIZE, \"width\": IMG_SIZE}),\n",
    "            iaa.Sequential([\n",
    "                iaa.Fliplr(0.5),\n",
    "                iaa.Affine(\n",
    "                    rotate=(-20, 20),\n",
    "                )\n",
    "            ], random_order=True) # apply augmenters in random order\n",
    "        ], random_order=False)\n",
    "    else: #ie.val\n",
    "        seq = iaa.Sequential([\n",
    "            iaa.Scale({\"height\": IMG_SIZE, \"width\": IMG_SIZE}),\n",
    "        ], random_order=False)\n",
    "#     seq = iaa.Sequential([\n",
    "#                 iaa.Scale({\"height\": IMG_SIZE, \"width\": IMG_SIZE}),\n",
    "#             ], random_order=False)\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "4c926ebe55fb37a7bfe3ffb2d6da2b145fb5f747"
   },
   "outputs": [],
   "source": [
    "train_set, val_set = train_test_split(annotations, test_size=VAL_SIZE, random_state=42)\n",
    "\n",
    "composed = {}\n",
    "composed['train'] = transforms.Compose([ImgTfm(aug_pipline=get_aug_pipline(img_size=IMG_SIZE, mode = 'train'))])\n",
    "composed['val'] = transforms.Compose([ImgTfm(aug_pipline=get_aug_pipline(img_size=IMG_SIZE, mode = 'val'))])\n",
    "\n",
    "image_datasets = {'train': ProteinDataset(train_set, img_path = PATH_TRAIN_IMG, transform=composed['train']),\n",
    "                 'val': ProteinDataset(val_set, img_path = PATH_TRAIN_IMG, transform=composed['val'])}\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, drop_last=True)\n",
    "              for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "e8c442abb721b3f65c4a7b076232757725311b2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 24857, 'val': 6215}\n"
     ]
    }
   ],
   "source": [
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "print(dataset_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "610b2e7a2e2b430673460bfd21e2940eca4a83b1"
   },
   "outputs": [],
   "source": [
    "#test dataset\n",
    "# ix = 10\n",
    "# tmp_img, tmp_tags  = image_datasets['train'][ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "116981e26006844aba80864a76434c9b0a445188"
   },
   "outputs": [],
   "source": [
    "#test dataloader\n",
    "# tmp_img, tmp_tags = next(iter(dataloaders['train']))\n",
    "# print('tmp_img shape: {}\\ntmp_tags: shape {}'.format(tmp_img.shape, tmp_tags.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "53926b18c61ec3d5f167ee905fcde5c0dc9289f5"
   },
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "    def forward(self, x): \n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "class RnetBackbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = self._prep_backbone()\n",
    "        \n",
    "    def _prep_backbone(self):     \n",
    "        base_model = models.resnet34(pretrained=True)\n",
    "        removed = list(base_model.children())[1:-2]\n",
    "        backbone = nn.Sequential(*removed)\n",
    "#         for param in backbone.parameters():\n",
    "#             param.require_grad = False\n",
    "        return backbone\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x\n",
    "\n",
    "class CustomHead(nn.Module):\n",
    "    def __init__(self, num_class):\n",
    "        super().__init__()\n",
    "        self.num_class = num_class\n",
    "        \n",
    "        self.flatten = Flatten()\n",
    "        self.relu_1 = nn.ReLU()\n",
    "        self.dropout_1 = nn.Dropout(p=0.5)\n",
    "        self.fc_2 = nn.Linear(512 * 7 * 7, 256)\n",
    "        self.relu_2 = nn.ReLU()\n",
    "        self.batchnorm_2 = nn.BatchNorm1d(256)\n",
    "        self.dropout_2 = nn.Dropout(p=0.5)\n",
    "        self.fc_3 = nn.Linear(256, self.num_class)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu_1(x)\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.fc_2(x)\n",
    "        x = self.relu_2(x)\n",
    "        x = self.batchnorm_2(x)\n",
    "        x = self.dropout_2(x)\n",
    "        x = self.fc_3(x)\n",
    "        return x\n",
    "\n",
    "# class CustomEntry(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.conv_1 = nn.Conv2d(in_channels=4, out_channels=64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "#         nn.init.kaiming_normal_(self.conv_1.weight, mode='fan_out', nonlinearity='relu')\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv_1(x)\n",
    "#         return x\n",
    "\n",
    "class CustomEntry(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_1 = self._prep_layers()\n",
    "        \n",
    "    def _prep_layers(self):\n",
    "        model = models.resnet34(pretrained=True)\n",
    "        original_entry_w = torch.tensor(list(model.children())[0].weight)\n",
    "        new_entry_w = torch.cat([original_entry_w, torch.zeros(size = (64,1,7,7))], 1)\n",
    "        \n",
    "        conv_1 = nn.Conv2d(in_channels=4, out_channels=64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        conv_1.weight=conv_1.weight = torch.nn.Parameter(new_entry_w)\n",
    "        return conv_1\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_1(x)\n",
    "        return x\n",
    "    \n",
    "class CustomNet(nn.Module):\n",
    "    def __init__(self, num_class):\n",
    "        super().__init__()\n",
    "        self.custom_entry = CustomEntry()\n",
    "        self.backbone = RnetBackbone()\n",
    "        self.custom_head = CustomHead(num_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.custom_entry(x)\n",
    "        x = self.backbone(x)\n",
    "        x = self.custom_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "4ef191d711342bf7bce65d943f8629d03bcf26c1"
   },
   "outputs": [],
   "source": [
    "class F1Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        #f1 loss\n",
    "#         #prep y_true\n",
    "        y_true = y_true.float()\n",
    "\n",
    "        #prep y_pred\n",
    "        y_pred = torch.tensor(data = (torch.sigmoid(y_pred).ge(THRESHOLD)), dtype=torch.float, device=DEVICE, requires_grad=True)\n",
    "\n",
    "        #calculate loss\n",
    "        tp = (y_true * y_pred).sum(0).float()\n",
    "        # tn = ((1-y_true) * (1-y_pred)).sum(0).float()\n",
    "        fp = ((1-y_true) * y_pred).sum(0).float()\n",
    "        fn = (y_true * (1-y_pred)).sum(0).float()\n",
    "\n",
    "        p = tp / (tp + fp)\n",
    "        r = tp / (tp + fn)\n",
    "\n",
    "        f1 = 2*p*r / (p+r)\n",
    "        f1[torch.isnan(f1)] = 0\n",
    "        f1_loss = 1-f1.mean()\n",
    "#         print(f1_loss)\n",
    "        return f1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        target = target.float()\n",
    "        \n",
    "        if not (target.size() == input.size()):\n",
    "            raise ValueError(\"Target size ({}) must be the same as input size ({})\"\n",
    "                             .format(target.size(), input.size()))\n",
    "\n",
    "        max_val = (-input).clamp(min=0)\n",
    "        loss = input - input * target + max_val + \\\n",
    "            ((-max_val).exp() + (-input - max_val).exp()).log()\n",
    "\n",
    "        invprobs = F.logsigmoid(-input * (target * 2.0 - 1.0))\n",
    "        loss = (invprobs * self.gamma).exp() * loss\n",
    "        \n",
    "        return loss.sum(dim=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "34c786807f448f63f4244537f13ebca2a6b44ee2"
   },
   "outputs": [],
   "source": [
    "def prep_stats(y_pred, y_true):\n",
    "    #prep y_true\n",
    "    y_true_tfm = y_true.cpu().numpy().astype('uint8')\n",
    "    \n",
    "    #prep y_pred khot\n",
    "    y_pred_tfm = (torch.sigmoid(y_pred) > THRESHOLD).cpu().numpy().astype('uint8')\n",
    "    \n",
    "    return y_pred_tfm, y_true_tfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "7df1b550cc9567063a38437001eff118004b0370"
   },
   "outputs": [],
   "source": [
    "def calc_stats(y_pred, y_true, stats = 'accurancy'):\n",
    "    if stats == 'accuracy':\n",
    "        stat_value = accuracy_score(y_true, y_pred)\n",
    "    elif stats == 'precision':\n",
    "        stat_value = precision_score(y_true, y_pred, average = 'macro')\n",
    "    elif stats == 'recall':\n",
    "        stat_value = recall_score(y_true, y_pred, average = 'macro')\n",
    "    elif stats == 'f1':\n",
    "        stat_value = f1_score(y_true, y_pred, average = 'macro')\n",
    "    else:\n",
    "        stat_value = 0\n",
    "    return stat_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "467bc143b3e21ddb3674f77f2c78287c9bc3684c"
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=5, init_steps = 0):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_f1 = 0.0\n",
    "    steps = init_steps\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            running_y_true = []\n",
    "            running_y_pred = []\n",
    "            \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, targets in dataloaders[phase]:\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                targets= targets.to(DEVICE)\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    \n",
    "                    y_pred_tfm, y_true_tfm = prep_stats(outputs, targets)\n",
    "                    running_y_pred.append(y_pred_tfm)\n",
    "                    running_y_true.append(y_true_tfm)\n",
    "                    \n",
    "                    #export step stats duing training phase\n",
    "                    if phase == 'train':\n",
    "                        record_tb(phase, 'loss', loss.cpu().data.numpy(), steps)\n",
    "                        record_tb(phase, 'accuracy', calc_stats(y_pred_tfm, y_true_tfm, stats = 'accurancy'), steps)\n",
    "                        record_tb(phase, 'precision', calc_stats(y_pred_tfm, y_true_tfm, stats = 'precision'), steps)\n",
    "                        record_tb(phase, 'recall', calc_stats(y_pred_tfm, y_true_tfm, stats = 'recall'), steps)\n",
    "                        record_tb(phase, 'f1', calc_stats(y_pred_tfm, y_true_tfm, stats = 'f1'), steps)\n",
    "                        steps += 1\n",
    "                        \n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            #calc epoch stats\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = accuracy_score(np.vstack(running_y_true), np.vstack(running_y_pred))\n",
    "            epoch_precision = precision_score(np.vstack(running_y_true), np.vstack(running_y_pred), average = 'macro')\n",
    "            epoch_recall = recall_score(np.vstack(running_y_true), np.vstack(running_y_pred), average = 'macro')\n",
    "            epoch_f1 = f1_score(np.vstack(running_y_true), np.vstack(running_y_pred), average = 'macro')\n",
    "            \n",
    "            #export epoch stats duing training phase\n",
    "            if phase == 'val':\n",
    "                record_tb(phase, 'loss', epoch_loss, steps)\n",
    "                record_tb(phase, 'accuracy', epoch_acc, steps)\n",
    "                record_tb(phase, 'precision', epoch_precision, steps)\n",
    "                record_tb(phase, 'recall', epoch_recall, steps)\n",
    "                record_tb(phase, 'f1', epoch_f1, steps)\n",
    "            \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f} Percision: {:.4f} Recall {:.4f} F1 {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc, epoch_precision, epoch_recall, epoch_f1))\n",
    "\n",
    "            # deep copy the model\n",
    "#             if phase == 'val' and epoch_acc > best_acc:\n",
    "#                 best_acc = epoch_acc\n",
    "#                 best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "#     print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "#     model.load_state_dict(best_model_wts)\n",
    "    return model, steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_ft = CustomNet(num_class=NUM_CLASSES)\n",
    "model_ft = model_ft.to(DEVICE)\n",
    "\n",
    "criterion = FocalLoss()\n",
    "\n",
    "steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spacor/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/spacor/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/spacor/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/spacor/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.3789 Acc: 0.0466 Percision: 0.0840 Recall 0.0379 F1 0.0439\n",
      "val Loss: 1.9596 Acc: 0.0678 Percision: 0.0831 Recall 0.0403 F1 0.0476\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.2277 Acc: 0.0644 Percision: 0.1617 Recall 0.0379 F1 0.0460\n",
      "val Loss: 1.6689 Acc: 0.0638 Percision: 0.1539 Recall 0.0363 F1 0.0466\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.2021 Acc: 0.0725 Percision: 0.2152 Recall 0.0429 F1 0.0519\n",
      "val Loss: 1.2471 Acc: 0.0783 Percision: 0.1378 Recall 0.0394 F1 0.0462\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.1898 Acc: 0.0785 Percision: 0.1864 Recall 0.0466 F1 0.0578\n",
      "val Loss: 1.2776 Acc: 0.0793 Percision: 0.1891 Recall 0.0432 F1 0.0541\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.1708 Acc: 0.0871 Percision: 0.2526 Recall 0.0508 F1 0.0640\n",
      "val Loss: 1.1451 Acc: 0.0920 Percision: 0.1389 Recall 0.0466 F1 0.0553\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.1586 Acc: 0.0925 Percision: 0.2723 Recall 0.0551 F1 0.0698\n",
      "val Loss: 1.2077 Acc: 0.0863 Percision: 0.1565 Recall 0.0499 F1 0.0684\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.1502 Acc: 0.0971 Percision: 0.2307 Recall 0.0587 F1 0.0758\n",
      "val Loss: 1.2064 Acc: 0.0978 Percision: 0.2110 Recall 0.0511 F1 0.0658\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.1442 Acc: 0.1002 Percision: 0.2498 Recall 0.0606 F1 0.0792\n",
      "val Loss: 1.3057 Acc: 0.0997 Percision: 0.2040 Recall 0.0538 F1 0.0698\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.1312 Acc: 0.1063 Percision: 0.2817 Recall 0.0667 F1 0.0878\n",
      "val Loss: 1.2097 Acc: 0.0757 Percision: 0.2062 Recall 0.0414 F1 0.0572\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.1283 Acc: 0.1063 Percision: 0.2923 Recall 0.0687 F1 0.0909\n",
      "val Loss: 1.0974 Acc: 0.0966 Percision: 0.3130 Recall 0.0672 F1 0.0912\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.0874 Acc: 0.1160 Percision: 0.3367 Recall 0.0760 F1 0.1009\n",
      "val Loss: 1.1184 Acc: 0.1234 Percision: 0.2392 Recall 0.0760 F1 0.1001\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0765 Acc: 0.1209 Percision: 0.3627 Recall 0.0788 F1 0.1051\n",
      "val Loss: 1.1707 Acc: 0.1211 Percision: 0.2578 Recall 0.0716 F1 0.0963\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 1.0703 Acc: 0.1242 Percision: 0.3390 Recall 0.0798 F1 0.1059\n",
      "val Loss: 1.1317 Acc: 0.1327 Percision: 0.3408 Recall 0.0851 F1 0.1118\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 1.0679 Acc: 0.1244 Percision: 0.3453 Recall 0.0825 F1 0.1093\n",
      "val Loss: 1.2622 Acc: 0.1202 Percision: 0.3332 Recall 0.0751 F1 0.1019\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 1.0648 Acc: 0.1277 Percision: 0.3973 Recall 0.0829 F1 0.1103\n",
      "val Loss: 1.2486 Acc: 0.1310 Percision: 0.3456 Recall 0.0815 F1 0.1100\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 1.0588 Acc: 0.1291 Percision: 0.3531 Recall 0.0847 F1 0.1121\n",
      "val Loss: 1.2031 Acc: 0.1348 Percision: 0.3984 Recall 0.0825 F1 0.1103\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 1.0586 Acc: 0.1301 Percision: 0.3452 Recall 0.0872 F1 0.1161\n",
      "val Loss: 2.2469 Acc: 0.1300 Percision: 0.3366 Recall 0.0798 F1 0.1087\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 1.0551 Acc: 0.1329 Percision: 0.3655 Recall 0.0881 F1 0.1175\n",
      "val Loss: 1.0698 Acc: 0.1252 Percision: 0.3088 Recall 0.0777 F1 0.1073\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 1.0528 Acc: 0.1347 Percision: 0.4243 Recall 0.0898 F1 0.1205\n",
      "val Loss: 1.1260 Acc: 0.1389 Percision: 0.3349 Recall 0.0903 F1 0.1208\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 1.0511 Acc: 0.1360 Percision: 0.3800 Recall 0.0904 F1 0.1205\n",
      "val Loss: 1.0530 Acc: 0.1414 Percision: 0.3339 Recall 0.0875 F1 0.1174\n",
      "\n",
      "Training complete in 101m 52s\n"
     ]
    }
   ],
   "source": [
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.01)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 10 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.1)\n",
    "\n",
    "#Freeze backbone\n",
    "for param in model_ft.backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_ft, steps = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=20, init_steps=steps)\n",
    "\n",
    "#save intermediate model\n",
    "torch.save(model_ft.state_dict(), 'intermediate_1_11052018.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spacor/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/spacor/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/spacor/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/spacor/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.1502 Acc: 0.1121 Percision: 0.2751 Recall 0.0655 F1 0.0861\n",
      "val Loss: 2.2840 Acc: 0.1395 Percision: 0.2242 Recall 0.1016 F1 0.1285\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 1.0876 Acc: 0.1460 Percision: 0.3117 Recall 0.0932 F1 0.1218\n",
      "val Loss: 1.5962 Acc: 0.1456 Percision: 0.2088 Recall 0.0877 F1 0.1114\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.0600 Acc: 0.1591 Percision: 0.3330 Recall 0.1093 F1 0.1434\n",
      "val Loss: 1.0473 Acc: 0.1683 Percision: 0.2715 Recall 0.1151 F1 0.1472\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.0432 Acc: 0.1691 Percision: 0.3295 Recall 0.1171 F1 0.1543\n",
      "val Loss: 1.3362 Acc: 0.1764 Percision: 0.2947 Recall 0.1168 F1 0.1488\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.0272 Acc: 0.1790 Percision: 0.3535 Recall 0.1262 F1 0.1668\n",
      "val Loss: 1.2668 Acc: 0.1946 Percision: 0.3602 Recall 0.1266 F1 0.1611\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.0163 Acc: 0.1828 Percision: 0.3712 Recall 0.1313 F1 0.1737\n",
      "val Loss: 0.9970 Acc: 0.2030 Percision: 0.3672 Recall 0.1356 F1 0.1683\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.0060 Acc: 0.1915 Percision: 0.3768 Recall 0.1375 F1 0.1814\n",
      "val Loss: 1.0021 Acc: 0.1885 Percision: 0.3985 Recall 0.1171 F1 0.1585\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 0.9945 Acc: 0.1974 Percision: 0.4368 Recall 0.1487 F1 0.1977\n",
      "val Loss: 1.0012 Acc: 0.2049 Percision: 0.3696 Recall 0.1279 F1 0.1676\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 0.9829 Acc: 0.2038 Percision: 0.4171 Recall 0.1527 F1 0.2023\n",
      "val Loss: 0.9911 Acc: 0.2113 Percision: 0.4379 Recall 0.1419 F1 0.1827\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 0.9747 Acc: 0.2084 Percision: 0.4364 Recall 0.1577 F1 0.2082\n",
      "val Loss: 0.9828 Acc: 0.2089 Percision: 0.4084 Recall 0.1531 F1 0.1978\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 0.9085 Acc: 0.2396 Percision: 0.5296 Recall 0.1839 F1 0.2415\n",
      "val Loss: 2.0831 Acc: 0.2465 Percision: 0.4443 Recall 0.1720 F1 0.2237\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 0.8943 Acc: 0.2523 Percision: 0.5375 Recall 0.1926 F1 0.2522\n",
      "val Loss: 3.1190 Acc: 0.2492 Percision: 0.4366 Recall 0.1722 F1 0.2231\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 0.8833 Acc: 0.2588 Percision: 0.5001 Recall 0.1972 F1 0.2571\n",
      "val Loss: 5.1703 Acc: 0.2492 Percision: 0.4793 Recall 0.1773 F1 0.2287\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 0.8729 Acc: 0.2655 Percision: 0.6296 Recall 0.2176 F1 0.2883\n",
      "val Loss: 1.9459 Acc: 0.2523 Percision: 0.4475 Recall 0.1776 F1 0.2289\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 0.8684 Acc: 0.2665 Percision: 0.4894 Recall 0.2054 F1 0.2652\n",
      "val Loss: 1.3816 Acc: 0.2466 Percision: 0.4687 Recall 0.1781 F1 0.2335\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.8604 Acc: 0.2709 Percision: 0.5235 Recall 0.2096 F1 0.2723\n",
      "val Loss: 1.7138 Acc: 0.2558 Percision: 0.4325 Recall 0.1862 F1 0.2383\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.8546 Acc: 0.2736 Percision: 0.5554 Recall 0.2203 F1 0.2868\n",
      "val Loss: 1.5347 Acc: 0.2584 Percision: 0.4644 Recall 0.1882 F1 0.2414\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.8485 Acc: 0.2778 Percision: 0.5923 Recall 0.2289 F1 0.2990\n",
      "val Loss: 2.2552 Acc: 0.2540 Percision: 0.4190 Recall 0.1880 F1 0.2393\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.8434 Acc: 0.2816 Percision: 0.5680 Recall 0.2355 F1 0.3051\n",
      "val Loss: 0.9266 Acc: 0.2550 Percision: 0.4723 Recall 0.1853 F1 0.2408\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.8387 Acc: 0.2827 Percision: 0.5907 Recall 0.2332 F1 0.3038\n",
      "val Loss: 0.9256 Acc: 0.2577 Percision: 0.4538 Recall 0.1899 F1 0.2432\n",
      "\n",
      "Training complete in 61m 50s\n"
     ]
    }
   ],
   "source": [
    "#freeze entry\n",
    "for param in model_ft.custom_entry.parameters():\n",
    "    param.requires_grad = False\n",
    "#unfreeze last covn of backbnoe\n",
    "for param in list(list(model_ft.backbone.children())[0][-1].parameters()):\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Different learning rate for different layers\n",
    "# optimizer_ft = optim.Adam([\n",
    "#     {'params': model_ft.custom_entry.parameters()},\n",
    "#     {'params': model_ft.backbone.parameters(), 'lr': 0.001},\n",
    "#     {'params': model_ft.custom_head.parameters()},\n",
    "#     ]\n",
    "# )\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.01)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 10 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.1)\n",
    "\n",
    "model_ft, steps = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=20, init_steps=steps)\n",
    "\n",
    "#save intermediate model\n",
    "torch.save(model_ft.state_dict(), 'intermediate_2_11052018.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spacor/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/spacor/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/spacor/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/spacor/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.2779 Acc: 0.0424 Percision: 0.1719 Recall 0.0265 F1 0.0358\n",
      "val Loss: 1.1902 Acc: 0.0717 Percision: 0.1115 Recall 0.0352 F1 0.0439\n",
      "\n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 1.1595 Acc: 0.0978 Percision: 0.2367 Recall 0.0605 F1 0.0786\n",
      "val Loss: 1.2426 Acc: 0.0854 Percision: 0.1416 Recall 0.0660 F1 0.0813\n",
      "\n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 1.0987 Acc: 0.1289 Percision: 0.3257 Recall 0.0852 F1 0.1109\n",
      "val Loss: 1.1224 Acc: 0.1777 Percision: 0.1795 Recall 0.1179 F1 0.1254\n",
      "\n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 1.0589 Acc: 0.1574 Percision: 0.2942 Recall 0.1063 F1 0.1389\n",
      "val Loss: 1.0359 Acc: 0.1757 Percision: 0.2732 Recall 0.1406 F1 0.1637\n",
      "\n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 1.0220 Acc: 0.1796 Percision: 0.3473 Recall 0.1263 F1 0.1631\n",
      "val Loss: 1.1752 Acc: 0.1949 Percision: 0.2998 Recall 0.1208 F1 0.1296\n",
      "\n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.9998 Acc: 0.1970 Percision: 0.3651 Recall 0.1407 F1 0.1821\n",
      "val Loss: 1.0437 Acc: 0.2033 Percision: 0.3252 Recall 0.1466 F1 0.1656\n",
      "\n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.9727 Acc: 0.2135 Percision: 0.3770 Recall 0.1548 F1 0.1995\n",
      "val Loss: 0.9778 Acc: 0.2112 Percision: 0.3510 Recall 0.1635 F1 0.1988\n",
      "\n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.9530 Acc: 0.2235 Percision: 0.4091 Recall 0.1709 F1 0.2199\n",
      "val Loss: 0.9216 Acc: 0.2645 Percision: 0.4145 Recall 0.1905 F1 0.2238\n",
      "\n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.9404 Acc: 0.2319 Percision: 0.4027 Recall 0.1764 F1 0.2257\n",
      "val Loss: 1.0149 Acc: 0.1965 Percision: 0.3788 Recall 0.1521 F1 0.1928\n",
      "\n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.9284 Acc: 0.2401 Percision: 0.4077 Recall 0.1849 F1 0.2357\n",
      "val Loss: 0.8945 Acc: 0.2692 Percision: 0.4120 Recall 0.1780 F1 0.2259\n",
      "\n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.8565 Acc: 0.2814 Percision: 0.5111 Recall 0.2185 F1 0.2779\n",
      "val Loss: 0.8251 Acc: 0.3044 Percision: 0.4818 Recall 0.2165 F1 0.2707\n",
      "\n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.8315 Acc: 0.2979 Percision: 0.5566 Recall 0.2296 F1 0.2896\n",
      "val Loss: 0.8148 Acc: 0.3133 Percision: 0.5327 Recall 0.2309 F1 0.2853\n",
      "\n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.8261 Acc: 0.2998 Percision: 0.5141 Recall 0.2315 F1 0.2889\n",
      "val Loss: 0.8109 Acc: 0.3168 Percision: 0.5148 Recall 0.2241 F1 0.2784\n",
      "\n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.8199 Acc: 0.3052 Percision: 0.5785 Recall 0.2379 F1 0.2964\n",
      "val Loss: 0.8087 Acc: 0.3235 Percision: 0.5092 Recall 0.2305 F1 0.2854\n",
      "\n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.8123 Acc: 0.3073 Percision: 0.6003 Recall 0.2448 F1 0.3082\n",
      "val Loss: 0.8047 Acc: 0.3159 Percision: 0.5227 Recall 0.2332 F1 0.2929\n",
      "\n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.8084 Acc: 0.3128 Percision: 0.5471 Recall 0.2443 F1 0.3032\n",
      "val Loss: 0.8005 Acc: 0.3252 Percision: 0.5250 Recall 0.2399 F1 0.2995\n",
      "\n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.8005 Acc: 0.3200 Percision: 0.5758 Recall 0.2499 F1 0.3102\n",
      "val Loss: 0.8020 Acc: 0.3215 Percision: 0.5317 Recall 0.2337 F1 0.2927\n",
      "\n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.7977 Acc: 0.3237 Percision: 0.5617 Recall 0.2512 F1 0.3112\n",
      "val Loss: 0.7972 Acc: 0.3281 Percision: 0.5326 Recall 0.2438 F1 0.3022\n",
      "\n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.7904 Acc: 0.3221 Percision: 0.5276 Recall 0.2521 F1 0.3107\n",
      "val Loss: 0.7949 Acc: 0.3380 Percision: 0.5313 Recall 0.2454 F1 0.3008\n",
      "\n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.7874 Acc: 0.3262 Percision: 0.5130 Recall 0.2546 F1 0.3141\n",
      "val Loss: 0.7946 Acc: 0.3318 Percision: 0.5275 Recall 0.2427 F1 0.3027\n",
      "\n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.7729 Acc: 0.3361 Percision: 0.6527 Recall 0.2707 F1 0.3370\n",
      "val Loss: 0.7838 Acc: 0.3404 Percision: 0.5309 Recall 0.2554 F1 0.3159\n",
      "\n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.7698 Acc: 0.3388 Percision: 0.5835 Recall 0.2661 F1 0.3287\n",
      "val Loss: 0.7812 Acc: 0.3394 Percision: 0.5343 Recall 0.2494 F1 0.3103\n",
      "\n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.7649 Acc: 0.3406 Percision: 0.5955 Recall 0.2733 F1 0.3394\n",
      "val Loss: 0.7807 Acc: 0.3404 Percision: 0.5476 Recall 0.2487 F1 0.3084\n",
      "\n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.7619 Acc: 0.3425 Percision: 0.6207 Recall 0.2781 F1 0.3471\n",
      "val Loss: 0.7818 Acc: 0.3392 Percision: 0.5409 Recall 0.2458 F1 0.3059\n",
      "\n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.7633 Acc: 0.3446 Percision: 0.6094 Recall 0.2767 F1 0.3425\n",
      "val Loss: 0.7813 Acc: 0.3388 Percision: 0.5433 Recall 0.2482 F1 0.3081\n",
      "\n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.7624 Acc: 0.3440 Percision: 0.5451 Recall 0.2693 F1 0.3309\n",
      "val Loss: 0.7783 Acc: 0.3413 Percision: 0.5456 Recall 0.2527 F1 0.3129\n",
      "\n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.7616 Acc: 0.3398 Percision: 0.6059 Recall 0.2720 F1 0.3358\n",
      "val Loss: 0.7799 Acc: 0.3450 Percision: 0.5440 Recall 0.2512 F1 0.3120\n",
      "\n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.7611 Acc: 0.3429 Percision: 0.6297 Recall 0.2773 F1 0.3438\n",
      "val Loss: 0.7807 Acc: 0.3426 Percision: 0.5444 Recall 0.2538 F1 0.3162\n",
      "\n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.7605 Acc: 0.3437 Percision: 0.6168 Recall 0.2780 F1 0.3432\n",
      "val Loss: 0.7795 Acc: 0.3429 Percision: 0.5475 Recall 0.2499 F1 0.3104\n",
      "\n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.7608 Acc: 0.3465 Percision: 0.5639 Recall 0.2750 F1 0.3377\n",
      "val Loss: 0.7797 Acc: 0.3420 Percision: 0.5439 Recall 0.2551 F1 0.3174\n",
      "\n",
      "Training complete in 204m 3s\n"
     ]
    }
   ],
   "source": [
    "for param in model_ft.backbone.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Different learning rate for different layers\n",
    "optimizer_ft = optim.Adam([\n",
    "    {'params': model_ft.custom_entry.parameters(), 'lr': 0.001},\n",
    "    {'params': model_ft.backbone.parameters(), 'lr': 0.001},\n",
    "    {'params': model_ft.custom_head.parameters()},\n",
    "    ]\n",
    ")\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.01)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 10 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.1)\n",
    "\n",
    "model_ft, steps = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=30, init_steps=steps)\n",
    "\n",
    "#save intermediate model\n",
    "torch.save(model_ft.state_dict(), 'intermediate_3_11052018.model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
