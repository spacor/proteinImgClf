{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, models\n",
    "\n",
    "from skimage import io\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches, patheffects\n",
    "\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "4248fd9c0afae8eaac391ab0d1895db2be249e75"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# base_path = r'../input'\n",
    "base_path = r'input'\n",
    "PATH_TRAIN_ANNO = os.path.join(base_path, 'train.csv')\n",
    "PATH_TRAIN_IMG = os.path.join(base_path, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "583ead53ccf420ab4290230a0a17cc3b6c62c74d"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from tensorboardX import SummaryWriter\n",
    "    USE_TENSORBOARD = True\n",
    "    writer = SummaryWriter()\n",
    "except:\n",
    "    USE_TENSORBOARD = False\n",
    "    print('No tensorboard X')\n",
    "\n",
    "def record_tb(phase, tag, value, global_step):\n",
    "    if USE_TENSORBOARD is True:\n",
    "        writer.add_scalar('{phase}/{tag}'.format(phase=phase, tag=tag), value, global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "7a95c899247b666f9235e3100a569744c46bf943"
   },
   "outputs": [],
   "source": [
    "# os.listdir(PATH_TRAIN_IMG)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "e85715fa2a9217474fc039c2b399c9b33490689a"
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 28\n",
    "MAX_TAGS = 5\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "VAL_SIZE =0.2\n",
    "THRESHOLD = 0.5\n",
    "SAMPLES = 1\n",
    "# DEVICE = torch.device(\"cpu\")\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_WORKERS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "7e81577b2c725960293a5cfabb29e0d46d66834c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample size: 31072\n",
      "[('00070df0-bbc3-11e8-b2bc-ac1f6b6435d0', [16, 0]),\n",
      " ('000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0', [7, 1, 2, 0]),\n",
      " ('000a9596-bbc4-11e8-b2bc-ac1f6b6435d0', [5])]\n"
     ]
    }
   ],
   "source": [
    "def get_transform_anno(annotation_path, img_path):\n",
    "    df = pd.read_csv(annotation_path)\n",
    "    annotations = []\n",
    "    for i, row in df.iterrows():\n",
    "        rcd_id = row['Id']\n",
    "        rcd_cate =  [int(j) for j in row['Target'].split()]\n",
    "        annotations.append((rcd_id, rcd_cate))\n",
    "    return annotations\n",
    "#get annotations\n",
    "annotations = get_transform_anno(PATH_TRAIN_ANNO, PATH_TRAIN_IMG)\n",
    "sample_size = int(len(annotations) * SAMPLES)\n",
    "print('sample size: {}'.format(sample_size))\n",
    "annotations = annotations[:sample_size]\n",
    "pprint(annotations[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "878d1a4d5d29884cc313258347df1ee630abae1a"
   },
   "outputs": [],
   "source": [
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, img_meta, img_path, transform = None):\n",
    "        self.img_meta = img_meta\n",
    "        self.transform = transform\n",
    "        self.channels = ['red', 'blue', 'yellow', 'green']\n",
    "        self.img_path = img_path\n",
    "        self.mlb = MultiLabelBinarizer(classes=range(0,NUM_CLASSES))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_meta)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id, img_tags= self.img_meta[idx]\n",
    "        ch = []\n",
    "        img_file_template = '{}_{}.png'\n",
    "        for c in self.channels:\n",
    "            ch.append(io.imread(os.path.join(self.img_path, img_file_template.format(img_id, c))))\n",
    "        img = np.stack(ch)\n",
    "\n",
    "        #augmentation\n",
    "        if bool(self.transform) is True:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        #binarize\n",
    "        img_tags = self.mlb.fit_transform([img_tags]).squeeze()\n",
    "        \n",
    "        #transform to tensor\n",
    "        img = torch.from_numpy(img).float()\n",
    "        img_tags = torch.from_numpy(img_tags)\n",
    "        \n",
    "        output = (img, img_tags)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "af583f0882e55fd31a3df230822ac86f99e25f4c"
   },
   "outputs": [],
   "source": [
    "class ImgTfm:\n",
    "    def __init__(self, aug_pipline = None):\n",
    "        self.seq = aug_pipline\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        \n",
    "#         seq_det = self.seq.to_deterministic()\n",
    "        \n",
    "        #augmentation\n",
    "        aug_img=img.copy().transpose((1, 2, 0))\n",
    "        aug_img = self.seq.augment_images([aug_img])[0]\n",
    "        aug_img=aug_img.transpose((2, 1, 0))\n",
    "        \n",
    "        #normalize\n",
    "        aug_img=aug_img/255\n",
    "        \n",
    "        return aug_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "7c0aeb64e2a7b651582a467d82079a2fda67c71b"
   },
   "outputs": [],
   "source": [
    "def get_aug_pipline(img_size, mode = 'train'):\n",
    "#     if mode == 'train':\n",
    "#         seq = iaa.Sequential([\n",
    "#             iaa.Scale({\"height\": IMG_SIZE, \"width\": IMG_SIZE}),\n",
    "#             iaa.Sequential([\n",
    "#                 iaa.Fliplr(0.5),\n",
    "#                 iaa.Affine(\n",
    "#                     rotate=(-20, 20),\n",
    "#                 )\n",
    "#             ], random_order=True) # apply augmenters in random order\n",
    "#         ], random_order=False)\n",
    "#     else: #ie.val\n",
    "#         seq = iaa.Sequential([\n",
    "#             iaa.Scale({\"height\": IMG_SIZE, \"width\": IMG_SIZE}),\n",
    "#         ], random_order=False)\n",
    "    seq = iaa.Sequential([\n",
    "                iaa.Scale({\"height\": IMG_SIZE, \"width\": IMG_SIZE}),\n",
    "            ], random_order=False)\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "4c926ebe55fb37a7bfe3ffb2d6da2b145fb5f747"
   },
   "outputs": [],
   "source": [
    "train_set, val_set = train_test_split(annotations, test_size=VAL_SIZE, random_state=42)\n",
    "\n",
    "composed = {}\n",
    "composed['train'] = transforms.Compose([ImgTfm(aug_pipline=get_aug_pipline(img_size=IMG_SIZE, mode = 'train'))])\n",
    "composed['val'] = transforms.Compose([ImgTfm(aug_pipline=get_aug_pipline(img_size=IMG_SIZE, mode = 'val'))])\n",
    "\n",
    "image_datasets = {'train': ProteinDataset(train_set, img_path = PATH_TRAIN_IMG, transform=composed['train']),\n",
    "                 'val': ProteinDataset(val_set, img_path = PATH_TRAIN_IMG, transform=composed['val'])}\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, drop_last=True)\n",
    "              for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "e8c442abb721b3f65c4a7b076232757725311b2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 24857, 'val': 6215}\n"
     ]
    }
   ],
   "source": [
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "print(dataset_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "610b2e7a2e2b430673460bfd21e2940eca4a83b1"
   },
   "outputs": [],
   "source": [
    "#test dataset\n",
    "# ix = 10\n",
    "# tmp_img, tmp_tags  = image_datasets['train'][ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "116981e26006844aba80864a76434c9b0a445188"
   },
   "outputs": [],
   "source": [
    "#test dataloader\n",
    "# tmp_img, tmp_tags = next(iter(dataloaders['train']))\n",
    "# print('tmp_img shape: {}\\ntmp_tags: shape {}'.format(tmp_img.shape, tmp_tags.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "53926b18c61ec3d5f167ee905fcde5c0dc9289f5"
   },
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "    def forward(self, x): \n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "class RnetBackbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = self._prep_backbone()\n",
    "        \n",
    "    def _prep_backbone(self):     \n",
    "        base_model = models.resnet34(pretrained=False)\n",
    "        removed = list(base_model.children())[1:-2]\n",
    "        backbone = nn.Sequential(*removed)\n",
    "#         for param in backbone.parameters():\n",
    "#             param.require_grad = False\n",
    "        return backbone\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x\n",
    "\n",
    "class CustomHead(nn.Module):\n",
    "    def __init__(self, num_class):\n",
    "        super().__init__()\n",
    "        self.num_class = num_class\n",
    "        \n",
    "        self.flatten = Flatten()\n",
    "        self.relu_1 = nn.ReLU()\n",
    "        self.dropout_1 = nn.Dropout(p=0.5)\n",
    "        self.fc_2 = nn.Linear(512 * 7 * 7, 256)\n",
    "        self.relu_2 = nn.ReLU()\n",
    "        self.batchnorm_2 = nn.BatchNorm1d(256)\n",
    "        self.dropout_2 = nn.Dropout(p=0.5)\n",
    "        self.fc_3 = nn.Linear(256, self.num_class)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu_1(x)\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.fc_2(x)\n",
    "        x = self.relu_2(x)\n",
    "        x = self.batchnorm_2(x)\n",
    "        x = self.dropout_2(x)\n",
    "        x = self.fc_3(x)\n",
    "        return x\n",
    "\n",
    "class CustomEntry(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_1 = nn.Conv2d(in_channels=4, out_channels=64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_1(x)\n",
    "        return x\n",
    "    \n",
    "class CustomNet(nn.Module):\n",
    "    def __init__(self, num_class):\n",
    "        super().__init__()\n",
    "        self.custom_entry = CustomEntry()\n",
    "        self.backbone = RnetBackbone()\n",
    "        self.custom_head = CustomHead(num_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.custom_entry(x)\n",
    "        x = self.backbone(x)\n",
    "        x = self.custom_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "4ef191d711342bf7bce65d943f8629d03bcf26c1"
   },
   "outputs": [],
   "source": [
    "class F1Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        #f1 loss\n",
    "#         #prep y_true\n",
    "        y_true = y_true.float()\n",
    "\n",
    "        #prep y_pred\n",
    "        y_pred = torch.tensor(data = (torch.sigmoid(y_pred).ge(THRESHOLD)), dtype=torch.float, device=DEVICE, requires_grad=True)\n",
    "\n",
    "        #calculate loss\n",
    "        tp = (y_true * y_pred).sum(0).float()\n",
    "        # tn = ((1-y_true) * (1-y_pred)).sum(0).float()\n",
    "        fp = ((1-y_true) * y_pred).sum(0).float()\n",
    "        fn = (y_true * (1-y_pred)).sum(0).float()\n",
    "\n",
    "        p = tp / (tp + fp)\n",
    "        r = tp / (tp + fn)\n",
    "\n",
    "        f1 = 2*p*r / (p+r)\n",
    "        f1[torch.isnan(f1)] = 0\n",
    "        f1_loss = 1-f1.mean()\n",
    "#         print(f1_loss)\n",
    "        return f1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        target = target.float()\n",
    "        \n",
    "        if not (target.size() == input.size()):\n",
    "            raise ValueError(\"Target size ({}) must be the same as input size ({})\"\n",
    "                             .format(target.size(), input.size()))\n",
    "\n",
    "        max_val = (-input).clamp(min=0)\n",
    "        loss = input - input * target + max_val + \\\n",
    "            ((-max_val).exp() + (-input - max_val).exp()).log()\n",
    "\n",
    "        invprobs = F.logsigmoid(-input * (target * 2.0 - 1.0))\n",
    "        loss = (invprobs * self.gamma).exp() * loss\n",
    "        \n",
    "        return loss.sum(dim=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "34c786807f448f63f4244537f13ebca2a6b44ee2"
   },
   "outputs": [],
   "source": [
    "def prep_stats(y_pred, y_true):\n",
    "    #prep y_true\n",
    "    y_true_tfm = y_true.cpu().numpy().astype('uint8')\n",
    "    \n",
    "    #prep y_pred khot\n",
    "    y_pred_tfm = (torch.sigmoid(y_pred) > THRESHOLD).cpu().numpy().astype('uint8')\n",
    "    \n",
    "    return y_pred_tfm, y_true_tfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "7df1b550cc9567063a38437001eff118004b0370"
   },
   "outputs": [],
   "source": [
    "def calc_stats(y_pred, y_true, stats = 'accurancy'):\n",
    "    if stats == 'accuracy':\n",
    "        stat_value = accuracy_score(y_true, y_pred)\n",
    "    elif stats == 'precision':\n",
    "        stat_value = precision_score(y_true, y_pred, average = 'macro')\n",
    "    elif stats == 'recall':\n",
    "        stat_value = recall_score(y_true, y_pred, average = 'macro')\n",
    "    elif stats == 'f1':\n",
    "        stat_value = f1_score(y_true, y_pred, average = 'macro')\n",
    "    else:\n",
    "        stat_value = 0\n",
    "    return stat_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "467bc143b3e21ddb3674f77f2c78287c9bc3684c"
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=5):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_f1 = 0.0\n",
    "    steps = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            running_y_true = []\n",
    "            running_y_pred = []\n",
    "            \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, targets in dataloaders[phase]:\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                targets= targets.to(DEVICE)\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    \n",
    "                    y_pred_tfm, y_true_tfm = prep_stats(outputs, targets)\n",
    "                    running_y_pred.append(y_pred_tfm)\n",
    "                    running_y_true.append(y_true_tfm)\n",
    "                    \n",
    "                    #export step stats duing training phase\n",
    "                    if phase == 'train':\n",
    "                        record_tb(phase, 'loss', loss.cpu().data.numpy(), steps)\n",
    "                        record_tb(phase, 'accuracy', calc_stats(y_pred_tfm, y_true_tfm, stats = 'accurancy'), steps)\n",
    "                        record_tb(phase, 'precision', calc_stats(y_pred_tfm, y_true_tfm, stats = 'precision'), steps)\n",
    "                        record_tb(phase, 'recall', calc_stats(y_pred_tfm, y_true_tfm, stats = 'recall'), steps)\n",
    "                        record_tb(phase, 'f1', calc_stats(y_pred_tfm, y_true_tfm, stats = 'f1'), steps)\n",
    "                        steps += 1\n",
    "                        \n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            #calc epoch stats\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = accuracy_score(np.vstack(running_y_true), np.vstack(running_y_pred))\n",
    "            epoch_precision = precision_score(np.vstack(running_y_true), np.vstack(running_y_pred), average = 'macro')\n",
    "            epoch_recall = recall_score(np.vstack(running_y_true), np.vstack(running_y_pred), average = 'macro')\n",
    "            epoch_f1 = f1_score(np.vstack(running_y_true), np.vstack(running_y_pred), average = 'macro')\n",
    "            \n",
    "            #export epoch stats duing training phase\n",
    "            if phase == 'val':\n",
    "                record_tb(phase, 'loss', epoch_loss, steps)\n",
    "                record_tb(phase, 'accuracy', epoch_acc, steps)\n",
    "                record_tb(phase, 'precision', epoch_precision, steps)\n",
    "                record_tb(phase, 'recall', epoch_recall, steps)\n",
    "                record_tb(phase, 'f1', epoch_f1, steps)\n",
    "            \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f} Percision: {:.4f} Recall {:.4f} F1 {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc, epoch_precision, epoch_recall, epoch_f1))\n",
    "\n",
    "            # deep copy the model\n",
    "#             if phase == 'val' and epoch_acc > best_acc:\n",
    "#                 best_acc = epoch_acc\n",
    "#                 best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "#     print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "#     model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "be546ee708d7ccb01c9f876bb189db4818a7d12a"
   },
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_ft = CustomNet(num_class=NUM_CLASSES)\n",
    "model_ft = model_ft.to(DEVICE)\n",
    "\n",
    "criterion = FocalLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.01)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "a27461b7756ca489c66881be19cb1d0d54bf5d47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spacor/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/spacor/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/spacor/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/spacor/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.3950 Acc: 0.0295 Percision: 0.0774 Recall 0.0190 F1 0.0273\n",
      "val Loss: 1.2957 Acc: 0.0852 Percision: 0.0457 Recall 0.0392 F1 0.0331\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 1.2643 Acc: 0.0629 Percision: 0.1286 Recall 0.0331 F1 0.0377\n",
      "val Loss: 1.2287 Acc: 0.0643 Percision: 0.0725 Recall 0.0337 F1 0.0350\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 1.2433 Acc: 0.0675 Percision: 0.1151 Recall 0.0361 F1 0.0411\n",
      "val Loss: 1.2246 Acc: 0.1110 Percision: 0.0951 Recall 0.0525 F1 0.0486\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 1.2306 Acc: 0.0714 Percision: 0.1421 Recall 0.0385 F1 0.0450\n",
      "val Loss: 5.6512 Acc: 0.1041 Percision: 0.1167 Recall 0.0631 F1 0.0698\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 1.2120 Acc: 0.0784 Percision: 0.1476 Recall 0.0427 F1 0.0529\n",
      "val Loss: 1.2501 Acc: 0.1079 Percision: 0.0542 Recall 0.0576 F1 0.0541\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 1.2082 Acc: 0.0732 Percision: 0.1526 Recall 0.0420 F1 0.0509\n",
      "val Loss: 1.1962 Acc: 0.0810 Percision: 0.1472 Recall 0.0653 F1 0.0706\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 1.1888 Acc: 0.0863 Percision: 0.1687 Recall 0.0489 F1 0.0615\n",
      "val Loss: 1.1608 Acc: 0.0909 Percision: 0.1556 Recall 0.0449 F1 0.0499\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 1.1357 Acc: 0.1131 Percision: 0.2336 Recall 0.0638 F1 0.0825\n",
      "val Loss: 1.1250 Acc: 0.1426 Percision: 0.1904 Recall 0.0834 F1 0.0980\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 1.1070 Acc: 0.1301 Percision: 0.2540 Recall 0.0754 F1 0.0979\n",
      "val Loss: 1.1951 Acc: 0.1450 Percision: 0.1700 Recall 0.0756 F1 0.0843\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 1.0779 Acc: 0.1459 Percision: 0.2860 Recall 0.0906 F1 0.1198\n",
      "val Loss: 1.3749 Acc: 0.1653 Percision: 0.2380 Recall 0.0922 F1 0.1100\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 0.9789 Acc: 0.1940 Percision: 0.3716 Recall 0.1215 F1 0.1554\n",
      "val Loss: 1.1344 Acc: 0.2133 Percision: 0.3553 Recall 0.1246 F1 0.1607\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 0.9452 Acc: 0.2135 Percision: 0.3848 Recall 0.1381 F1 0.1758\n",
      "val Loss: 0.9151 Acc: 0.2368 Percision: 0.4651 Recall 0.1437 F1 0.1842\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 0.9222 Acc: 0.2293 Percision: 0.3963 Recall 0.1539 F1 0.1956\n",
      "val Loss: 0.9754 Acc: 0.2516 Percision: 0.4071 Recall 0.1560 F1 0.1967\n",
      "\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 0.9029 Acc: 0.2442 Percision: 0.4159 Recall 0.1651 F1 0.2103\n",
      "val Loss: 0.8961 Acc: 0.2677 Percision: 0.3748 Recall 0.1571 F1 0.1960\n",
      "\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 0.8817 Acc: 0.2564 Percision: 0.4206 Recall 0.1777 F1 0.2244\n",
      "val Loss: 0.9219 Acc: 0.2793 Percision: 0.4250 Recall 0.1795 F1 0.2215\n",
      "\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 0.8562 Acc: 0.2709 Percision: 0.4291 Recall 0.1903 F1 0.2388\n",
      "val Loss: 0.8821 Acc: 0.2804 Percision: 0.4210 Recall 0.1810 F1 0.2216\n",
      "\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 0.8321 Acc: 0.2832 Percision: 0.4536 Recall 0.2028 F1 0.2520\n",
      "val Loss: 0.8838 Acc: 0.2808 Percision: 0.4476 Recall 0.1870 F1 0.2320\n",
      "\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 0.8094 Acc: 0.2944 Percision: 0.4685 Recall 0.2141 F1 0.2656\n",
      "val Loss: 0.9375 Acc: 0.2738 Percision: 0.4235 Recall 0.1923 F1 0.2383\n",
      "\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 0.7744 Acc: 0.3156 Percision: 0.5081 Recall 0.2338 F1 0.2875\n",
      "val Loss: 0.8801 Acc: 0.2940 Percision: 0.4665 Recall 0.1966 F1 0.2409\n",
      "\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 0.7367 Acc: 0.3401 Percision: 0.5954 Recall 0.2580 F1 0.3182\n",
      "val Loss: 0.9311 Acc: 0.2995 Percision: 0.4567 Recall 0.2086 F1 0.2486\n",
      "\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 0.6635 Acc: 0.3777 Percision: 0.6018 Recall 0.2798 F1 0.3396\n",
      "val Loss: 0.9053 Acc: 0.3032 Percision: 0.4415 Recall 0.2101 F1 0.2626\n",
      "\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 0.6352 Acc: 0.3969 Percision: 0.5894 Recall 0.2954 F1 0.3534\n",
      "val Loss: 0.9174 Acc: 0.3044 Percision: 0.4406 Recall 0.2107 F1 0.2612\n",
      "\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 0.6248 Acc: 0.4008 Percision: 0.5989 Recall 0.3006 F1 0.3581\n",
      "val Loss: 0.9361 Acc: 0.3106 Percision: 0.4421 Recall 0.2224 F1 0.2733\n",
      "\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 0.6073 Acc: 0.4151 Percision: 0.6458 Recall 0.3223 F1 0.3863\n",
      "val Loss: 0.9441 Acc: 0.3096 Percision: 0.4340 Recall 0.2250 F1 0.2763\n",
      "\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 0.5967 Acc: 0.4223 Percision: 0.6187 Recall 0.3262 F1 0.3870\n",
      "val Loss: 0.9639 Acc: 0.3070 Percision: 0.4377 Recall 0.2217 F1 0.2755\n",
      "\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 0.5891 Acc: 0.4295 Percision: 0.6205 Recall 0.3373 F1 0.4006\n",
      "val Loss: 0.9760 Acc: 0.3072 Percision: 0.4265 Recall 0.2199 F1 0.2687\n",
      "\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 0.5758 Acc: 0.4389 Percision: 0.6911 Recall 0.3472 F1 0.4150\n",
      "val Loss: 0.9969 Acc: 0.3090 Percision: 0.4298 Recall 0.2217 F1 0.2717\n",
      "\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 0.5646 Acc: 0.4454 Percision: 0.6243 Recall 0.3476 F1 0.4103\n",
      "val Loss: 1.0058 Acc: 0.3033 Percision: 0.4286 Recall 0.2251 F1 0.2736\n",
      "\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 0.5573 Acc: 0.4515 Percision: 0.6344 Recall 0.3566 F1 0.4202\n",
      "val Loss: 1.0135 Acc: 0.3102 Percision: 0.4280 Recall 0.2224 F1 0.2709\n",
      "\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 0.5426 Acc: 0.4646 Percision: 0.6875 Recall 0.3664 F1 0.4324\n",
      "val Loss: 1.0429 Acc: 0.3117 Percision: 0.4259 Recall 0.2283 F1 0.2776\n",
      "\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 0.5305 Acc: 0.4710 Percision: 0.6438 Recall 0.3839 F1 0.4502\n",
      "val Loss: 1.0311 Acc: 0.3117 Percision: 0.4233 Recall 0.2268 F1 0.2762\n",
      "\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 0.5281 Acc: 0.4702 Percision: 0.6695 Recall 0.3921 F1 0.4618\n",
      "val Loss: 1.0390 Acc: 0.3078 Percision: 0.4268 Recall 0.2266 F1 0.2767\n",
      "\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 0.5290 Acc: 0.4699 Percision: 0.6805 Recall 0.3904 F1 0.4611\n",
      "val Loss: 1.0353 Acc: 0.3070 Percision: 0.4239 Recall 0.2250 F1 0.2732\n",
      "\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 0.5259 Acc: 0.4742 Percision: 0.6361 Recall 0.3751 F1 0.4406\n",
      "val Loss: 1.0302 Acc: 0.3061 Percision: 0.4266 Recall 0.2209 F1 0.2711\n",
      "\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 0.5231 Acc: 0.4746 Percision: 0.6648 Recall 0.3785 F1 0.4453\n",
      "val Loss: 1.0601 Acc: 0.3096 Percision: 0.4190 Recall 0.2296 F1 0.2795\n",
      "\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 0.5249 Acc: 0.4705 Percision: 0.6452 Recall 0.3714 F1 0.4355\n",
      "val Loss: 1.0391 Acc: 0.3075 Percision: 0.4264 Recall 0.2265 F1 0.2772\n",
      "\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 0.5218 Acc: 0.4779 Percision: 0.6599 Recall 0.3865 F1 0.4550\n",
      "val Loss: 1.0363 Acc: 0.3080 Percision: 0.4241 Recall 0.2262 F1 0.2749\n",
      "\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 0.5212 Acc: 0.4750 Percision: 0.6590 Recall 0.3787 F1 0.4435\n",
      "val Loss: 1.0426 Acc: 0.3080 Percision: 0.4233 Recall 0.2244 F1 0.2742\n",
      "\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 0.5204 Acc: 0.4816 Percision: 0.6591 Recall 0.3790 F1 0.4461\n",
      "val Loss: 1.0658 Acc: 0.3086 Percision: 0.4215 Recall 0.2236 F1 0.2725\n",
      "\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 0.5209 Acc: 0.4785 Percision: 0.6686 Recall 0.3788 F1 0.4434\n",
      "val Loss: 1.0519 Acc: 0.3109 Percision: 0.4209 Recall 0.2246 F1 0.2744\n",
      "\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 0.5197 Acc: 0.4797 Percision: 0.6483 Recall 0.3757 F1 0.4396\n",
      "val Loss: 1.0535 Acc: 0.3072 Percision: 0.4227 Recall 0.2241 F1 0.2744\n",
      "\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 0.5163 Acc: 0.4802 Percision: 0.6483 Recall 0.3852 F1 0.4517\n",
      "val Loss: 1.0514 Acc: 0.3073 Percision: 0.4109 Recall 0.2294 F1 0.2803\n",
      "\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 0.5175 Acc: 0.4792 Percision: 0.6445 Recall 0.3821 F1 0.4487\n",
      "val Loss: 1.0595 Acc: 0.3106 Percision: 0.4242 Recall 0.2256 F1 0.2743\n",
      "\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 0.5160 Acc: 0.4822 Percision: 0.6429 Recall 0.3903 F1 0.4558\n",
      "val Loss: 1.0647 Acc: 0.3107 Percision: 0.4178 Recall 0.2273 F1 0.2786\n",
      "\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 0.5168 Acc: 0.4789 Percision: 0.6947 Recall 0.3906 F1 0.4628\n",
      "val Loss: 1.0646 Acc: 0.3033 Percision: 0.4247 Recall 0.2215 F1 0.2715\n",
      "\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 0.5177 Acc: 0.4798 Percision: 0.6591 Recall 0.3869 F1 0.4551\n",
      "val Loss: 1.0572 Acc: 0.3040 Percision: 0.4267 Recall 0.2227 F1 0.2732\n",
      "\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 0.5195 Acc: 0.4830 Percision: 0.6470 Recall 0.3774 F1 0.4411\n",
      "val Loss: 1.0582 Acc: 0.3056 Percision: 0.4187 Recall 0.2255 F1 0.2752\n",
      "\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 0.5198 Acc: 0.4783 Percision: 0.6377 Recall 0.3767 F1 0.4414\n",
      "val Loss: 1.0444 Acc: 0.3075 Percision: 0.4152 Recall 0.2211 F1 0.2699\n",
      "\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 0.5174 Acc: 0.4752 Percision: 0.6537 Recall 0.3774 F1 0.4423\n",
      "val Loss: 1.0941 Acc: 0.3046 Percision: 0.4120 Recall 0.2286 F1 0.2769\n",
      "\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 0.5141 Acc: 0.4816 Percision: 0.6951 Recall 0.3822 F1 0.4505\n",
      "val Loss: 1.0641 Acc: 0.3064 Percision: 0.4188 Recall 0.2266 F1 0.2767\n",
      "\n",
      "Training complete in 361m 10s\n"
     ]
    }
   ],
   "source": [
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "c6216757e2fd5f18f15c28d27f29427270f3789b"
   },
   "outputs": [],
   "source": [
    "# summary(model_ft, (4, IMG_SIZE, IMG_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomNet(\n",
       "  (custom_entry): CustomEntry(\n",
       "    (conv_1): Conv2d(4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  )\n",
       "  (backbone): RnetBackbone(\n",
       "    (backbone): Sequential(\n",
       "      (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): ReLU(inplace)\n",
       "      (2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (3): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (4): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (5): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (custom_head): CustomHead(\n",
       "    (flatten): Flatten()\n",
       "    (relu_1): ReLU()\n",
       "    (dropout_1): Dropout(p=0.5)\n",
       "    (fc_2): Linear(in_features=25088, out_features=256, bias=True)\n",
       "    (relu_2): ReLU()\n",
       "    (batchnorm_2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout_2): Dropout(p=0.5)\n",
       "    (fc_3): Linear(in_features=256, out_features=28, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
