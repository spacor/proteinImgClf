{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, models\n",
    "\n",
    "from skimage import io\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches, patheffects\n",
    "\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "4248fd9c0afae8eaac391ab0d1895db2be249e75"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# base_path = r'../input'\n",
    "base_path = r'input'\n",
    "PATH_TRAIN_ANNO = os.path.join(base_path, 'train.csv')\n",
    "PATH_TRAIN_IMG = os.path.join(base_path, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "583ead53ccf420ab4290230a0a17cc3b6c62c74d"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from tensorboardX import SummaryWriter\n",
    "    USE_TENSORBOARD = True\n",
    "    writer = SummaryWriter()\n",
    "except:\n",
    "    USE_TENSORBOARD = False\n",
    "    print('No tensorboard X')\n",
    "\n",
    "def record_tb(phase, tag, value, global_step):\n",
    "    if USE_TENSORBOARD is True:\n",
    "        writer.add_scalar('{phase}/{tag}'.format(phase=phase, tag=tag), value, global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "7a95c899247b666f9235e3100a569744c46bf943"
   },
   "outputs": [],
   "source": [
    "# os.listdir(PATH_TRAIN_IMG)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "e85715fa2a9217474fc039c2b399c9b33490689a"
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 28\n",
    "MAX_TAGS = 5\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "VAL_SIZE =0.2\n",
    "THRESHOLD = 0.5\n",
    "SAMPLES = 1\n",
    "# DEVICE = torch.device(\"cpu\")\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_WORKERS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "7e81577b2c725960293a5cfabb29e0d46d66834c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample size: 31072\n",
      "[('00070df0-bbc3-11e8-b2bc-ac1f6b6435d0', [16, 0]),\n",
      " ('000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0', [7, 1, 2, 0]),\n",
      " ('000a9596-bbc4-11e8-b2bc-ac1f6b6435d0', [5])]\n"
     ]
    }
   ],
   "source": [
    "def get_transform_anno(annotation_path, img_path):\n",
    "    df = pd.read_csv(annotation_path)\n",
    "    annotations = []\n",
    "    for i, row in df.iterrows():\n",
    "        rcd_id = row['Id']\n",
    "        rcd_cate =  [int(j) for j in row['Target'].split()]\n",
    "        annotations.append((rcd_id, rcd_cate))\n",
    "    return annotations\n",
    "#get annotations\n",
    "annotations = get_transform_anno(PATH_TRAIN_ANNO, PATH_TRAIN_IMG)\n",
    "sample_size = int(len(annotations) * SAMPLES)\n",
    "print('sample size: {}'.format(sample_size))\n",
    "annotations = annotations[:sample_size]\n",
    "pprint(annotations[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "878d1a4d5d29884cc313258347df1ee630abae1a"
   },
   "outputs": [],
   "source": [
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, img_meta, img_path, transform = None):\n",
    "        self.img_meta = img_meta\n",
    "        self.transform = transform\n",
    "        self.channels = ['red', 'blue', 'yellow', 'green']\n",
    "        self.img_path = img_path\n",
    "        self.mlb = MultiLabelBinarizer(classes=range(0,NUM_CLASSES))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_meta)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id, img_tags= self.img_meta[idx]\n",
    "        ch = []\n",
    "        img_file_template = '{}_{}.png'\n",
    "        for c in self.channels:\n",
    "            ch.append(io.imread(os.path.join(self.img_path, img_file_template.format(img_id, c))))\n",
    "        img = np.stack(ch)\n",
    "\n",
    "        #augmentation\n",
    "        if bool(self.transform) is True:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        #binarize\n",
    "        img_tags = self.mlb.fit_transform([img_tags]).squeeze()\n",
    "        \n",
    "        #transform to tensor\n",
    "        img = torch.from_numpy(img).float()\n",
    "        img_tags = torch.from_numpy(img_tags)\n",
    "        \n",
    "        output = (img, img_tags)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "af583f0882e55fd31a3df230822ac86f99e25f4c"
   },
   "outputs": [],
   "source": [
    "class ImgTfm:\n",
    "    def __init__(self, aug_pipline = None):\n",
    "        self.seq = aug_pipline\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        \n",
    "#         seq_det = self.seq.to_deterministic()\n",
    "        \n",
    "        #augmentation\n",
    "        aug_img=img.copy().transpose((1, 2, 0))\n",
    "        aug_img = self.seq.augment_images([aug_img])[0]\n",
    "        aug_img=aug_img.transpose((2, 1, 0))\n",
    "        \n",
    "        #normalize\n",
    "        aug_img=aug_img/255\n",
    "        \n",
    "        return aug_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "7c0aeb64e2a7b651582a467d82079a2fda67c71b"
   },
   "outputs": [],
   "source": [
    "def get_aug_pipline(img_size, mode = 'train'):\n",
    "    if mode == 'train':\n",
    "        seq = iaa.Sequential([\n",
    "            iaa.Scale({\"height\": IMG_SIZE, \"width\": IMG_SIZE}),\n",
    "            iaa.Sequential([\n",
    "#                 iaa.Fliplr(0.5),\n",
    "                iaa.Affine(\n",
    "                    rotate=(-20, 20),\n",
    "                )\n",
    "            ], random_order=True) # apply augmenters in random order\n",
    "        ], random_order=False)\n",
    "    else: #ie.val\n",
    "        seq = iaa.Sequential([\n",
    "            iaa.Scale({\"height\": IMG_SIZE, \"width\": IMG_SIZE}),\n",
    "        ], random_order=False)\n",
    "#     seq = iaa.Sequential([\n",
    "#                 iaa.Scale({\"height\": IMG_SIZE, \"width\": IMG_SIZE}),\n",
    "#             ], random_order=False)\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "4c926ebe55fb37a7bfe3ffb2d6da2b145fb5f747"
   },
   "outputs": [],
   "source": [
    "train_set, val_set = train_test_split(annotations, test_size=VAL_SIZE, random_state=42)\n",
    "\n",
    "composed = {}\n",
    "composed['train'] = transforms.Compose([ImgTfm(aug_pipline=get_aug_pipline(img_size=IMG_SIZE, mode = 'train'))])\n",
    "composed['val'] = transforms.Compose([ImgTfm(aug_pipline=get_aug_pipline(img_size=IMG_SIZE, mode = 'val'))])\n",
    "\n",
    "image_datasets = {'train': ProteinDataset(train_set, img_path = PATH_TRAIN_IMG, transform=composed['train']),\n",
    "                 'val': ProteinDataset(val_set, img_path = PATH_TRAIN_IMG, transform=composed['val'])}\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, drop_last=True)\n",
    "              for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "e8c442abb721b3f65c4a7b076232757725311b2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 24857, 'val': 6215}\n"
     ]
    }
   ],
   "source": [
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "print(dataset_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "610b2e7a2e2b430673460bfd21e2940eca4a83b1"
   },
   "outputs": [],
   "source": [
    "#test dataset\n",
    "# ix = 10\n",
    "# tmp_img, tmp_tags  = image_datasets['train'][ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "116981e26006844aba80864a76434c9b0a445188"
   },
   "outputs": [],
   "source": [
    "#test dataloader\n",
    "# tmp_img, tmp_tags = next(iter(dataloaders['train']))\n",
    "# print('tmp_img shape: {}\\ntmp_tags: shape {}'.format(tmp_img.shape, tmp_tags.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "53926b18c61ec3d5f167ee905fcde5c0dc9289f5"
   },
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "    def forward(self, x): \n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "class RnetBackbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = self._prep_backbone()\n",
    "        \n",
    "    def _prep_backbone(self):     \n",
    "        base_model = models.resnet34(pretrained=False)\n",
    "        removed = list(base_model.children())[1:-2]\n",
    "        backbone = nn.Sequential(*removed)\n",
    "#         for param in backbone.parameters():\n",
    "#             param.require_grad = False\n",
    "        return backbone\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x\n",
    "\n",
    "class CustomHead(nn.Module):\n",
    "    def __init__(self, num_class):\n",
    "        super().__init__()\n",
    "        self.num_class = num_class\n",
    "        \n",
    "        self.flatten = Flatten()\n",
    "        self.relu_1 = nn.ReLU()\n",
    "        self.dropout_1 = nn.Dropout(p=0.5)\n",
    "        self.fc_2 = nn.Linear(512 * 7 * 7, 256)\n",
    "        self.relu_2 = nn.ReLU()\n",
    "        self.batchnorm_2 = nn.BatchNorm1d(256)\n",
    "        self.dropout_2 = nn.Dropout(p=0.5)\n",
    "        self.fc_3 = nn.Linear(256, self.num_class)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu_1(x)\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.fc_2(x)\n",
    "        x = self.relu_2(x)\n",
    "        x = self.batchnorm_2(x)\n",
    "        x = self.dropout_2(x)\n",
    "        x = self.fc_3(x)\n",
    "        return x\n",
    "\n",
    "class CustomEntry(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_1 = nn.Conv2d(in_channels=4, out_channels=64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_1(x)\n",
    "        return x\n",
    "    \n",
    "class CustomNet(nn.Module):\n",
    "    def __init__(self, num_class):\n",
    "        super().__init__()\n",
    "        self.custom_entry = CustomEntry()\n",
    "        self.backbone = RnetBackbone()\n",
    "        self.custom_head = CustomHead(num_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.custom_entry(x)\n",
    "        x = self.backbone(x)\n",
    "        x = self.custom_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "4ef191d711342bf7bce65d943f8629d03bcf26c1"
   },
   "outputs": [],
   "source": [
    "class F1Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        #f1 loss\n",
    "#         #prep y_true\n",
    "        y_true = y_true.float()\n",
    "\n",
    "        #prep y_pred\n",
    "        y_pred = torch.tensor(data = (torch.sigmoid(y_pred).ge(THRESHOLD)), dtype=torch.float, device=DEVICE, requires_grad=True)\n",
    "\n",
    "        #calculate loss\n",
    "        tp = (y_true * y_pred).sum(0).float()\n",
    "        # tn = ((1-y_true) * (1-y_pred)).sum(0).float()\n",
    "        fp = ((1-y_true) * y_pred).sum(0).float()\n",
    "        fn = (y_true * (1-y_pred)).sum(0).float()\n",
    "\n",
    "        p = tp / (tp + fp)\n",
    "        r = tp / (tp + fn)\n",
    "\n",
    "        f1 = 2*p*r / (p+r)\n",
    "        f1[torch.isnan(f1)] = 0\n",
    "        f1_loss = 1-f1.mean()\n",
    "#         print(f1_loss)\n",
    "        return f1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        target = target.float()\n",
    "        \n",
    "        if not (target.size() == input.size()):\n",
    "            raise ValueError(\"Target size ({}) must be the same as input size ({})\"\n",
    "                             .format(target.size(), input.size()))\n",
    "\n",
    "        max_val = (-input).clamp(min=0)\n",
    "        loss = input - input * target + max_val + \\\n",
    "            ((-max_val).exp() + (-input - max_val).exp()).log()\n",
    "\n",
    "        invprobs = F.logsigmoid(-input * (target * 2.0 - 1.0))\n",
    "        loss = (invprobs * self.gamma).exp() * loss\n",
    "        \n",
    "        return loss.sum(dim=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "34c786807f448f63f4244537f13ebca2a6b44ee2"
   },
   "outputs": [],
   "source": [
    "def prep_stats(y_pred, y_true):\n",
    "    #prep y_true\n",
    "    y_true_tfm = y_true.cpu().numpy().astype('uint8')\n",
    "    \n",
    "    #prep y_pred khot\n",
    "    y_pred_tfm = (torch.sigmoid(y_pred) > THRESHOLD).cpu().numpy().astype('uint8')\n",
    "    \n",
    "    return y_pred_tfm, y_true_tfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "7df1b550cc9567063a38437001eff118004b0370"
   },
   "outputs": [],
   "source": [
    "def calc_stats(y_pred, y_true, stats = 'accurancy'):\n",
    "    if stats == 'accuracy':\n",
    "        stat_value = accuracy_score(y_true, y_pred)\n",
    "    elif stats == 'precision':\n",
    "        stat_value = precision_score(y_true, y_pred, average = 'macro')\n",
    "    elif stats == 'recall':\n",
    "        stat_value = recall_score(y_true, y_pred, average = 'macro')\n",
    "    elif stats == 'f1':\n",
    "        stat_value = f1_score(y_true, y_pred, average = 'macro')\n",
    "    else:\n",
    "        stat_value = 0\n",
    "    return stat_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "467bc143b3e21ddb3674f77f2c78287c9bc3684c"
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=5):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_f1 = 0.0\n",
    "    steps = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            running_y_true = []\n",
    "            running_y_pred = []\n",
    "            \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, targets in dataloaders[phase]:\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                targets= targets.to(DEVICE)\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    \n",
    "                    y_pred_tfm, y_true_tfm = prep_stats(outputs, targets)\n",
    "                    running_y_pred.append(y_pred_tfm)\n",
    "                    running_y_true.append(y_true_tfm)\n",
    "                    \n",
    "                    #export step stats duing training phase\n",
    "                    if phase == 'train':\n",
    "                        record_tb(phase, 'loss', loss.cpu().data.numpy(), steps)\n",
    "                        record_tb(phase, 'accuracy', calc_stats(y_pred_tfm, y_true_tfm, stats = 'accurancy'), steps)\n",
    "                        record_tb(phase, 'precision', calc_stats(y_pred_tfm, y_true_tfm, stats = 'precision'), steps)\n",
    "                        record_tb(phase, 'recall', calc_stats(y_pred_tfm, y_true_tfm, stats = 'recall'), steps)\n",
    "                        record_tb(phase, 'f1', calc_stats(y_pred_tfm, y_true_tfm, stats = 'f1'), steps)\n",
    "                        steps += 1\n",
    "                        \n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            #calc epoch stats\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = accuracy_score(np.vstack(running_y_true), np.vstack(running_y_pred))\n",
    "            epoch_precision = precision_score(np.vstack(running_y_true), np.vstack(running_y_pred), average = 'macro')\n",
    "            epoch_recall = recall_score(np.vstack(running_y_true), np.vstack(running_y_pred), average = 'macro')\n",
    "            epoch_f1 = f1_score(np.vstack(running_y_true), np.vstack(running_y_pred), average = 'macro')\n",
    "            \n",
    "            #export epoch stats duing training phase\n",
    "            if phase == 'val':\n",
    "                record_tb(phase, 'loss', epoch_loss, steps)\n",
    "                record_tb(phase, 'accuracy', epoch_acc, steps)\n",
    "                record_tb(phase, 'precision', epoch_precision, steps)\n",
    "                record_tb(phase, 'recall', epoch_recall, steps)\n",
    "                record_tb(phase, 'f1', epoch_f1, steps)\n",
    "            \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f} Percision: {:.4f} Recall {:.4f} F1 {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc, epoch_precision, epoch_recall, epoch_f1))\n",
    "\n",
    "            # deep copy the model\n",
    "#             if phase == 'val' and epoch_acc > best_acc:\n",
    "#                 best_acc = epoch_acc\n",
    "#                 best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "#     print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "#     model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "be546ee708d7ccb01c9f876bb189db4818a7d12a"
   },
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_ft = CustomNet(num_class=NUM_CLASSES)\n",
    "model_ft = model_ft.to(DEVICE)\n",
    "\n",
    "criterion = FocalLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.01)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "a27461b7756ca489c66881be19cb1d0d54bf5d47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spacor/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/spacor/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/spacor/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/spacor/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.3923 Acc: 0.0264 Percision: 0.0739 Recall 0.0171 F1 0.0256\n",
      "val Loss: 1.3271 Acc: 0.0408 Percision: 0.0828 Recall 0.0229 F1 0.0279\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 1.2784 Acc: 0.0540 Percision: 0.0740 Recall 0.0274 F1 0.0344\n",
      "val Loss: 1.2371 Acc: 0.0714 Percision: 0.0985 Recall 0.0380 F1 0.0438\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 1.2399 Acc: 0.0666 Percision: 0.0975 Recall 0.0369 F1 0.0432\n",
      "val Loss: 1.2049 Acc: 0.0913 Percision: 0.0614 Recall 0.0473 F1 0.0472\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 1.2162 Acc: 0.0746 Percision: 0.1547 Recall 0.0424 F1 0.0516\n",
      "val Loss: 1.7449 Acc: 0.0931 Percision: 0.1248 Recall 0.0470 F1 0.0553\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 1.1780 Acc: 0.0943 Percision: 0.2201 Recall 0.0525 F1 0.0664\n",
      "val Loss: 31.9080 Acc: 0.1268 Percision: 0.1822 Recall 0.0785 F1 0.0868\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 1.1439 Acc: 0.1150 Percision: 0.2543 Recall 0.0658 F1 0.0852\n",
      "val Loss: 1.4127 Acc: 0.1385 Percision: 0.2023 Recall 0.0785 F1 0.0995\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 1.1722 Acc: 0.0985 Percision: 0.2223 Recall 0.0560 F1 0.0739\n",
      "val Loss: 1.0954 Acc: 0.1405 Percision: 0.1949 Recall 0.0795 F1 0.0993\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 1.1146 Acc: 0.1252 Percision: 0.2367 Recall 0.0766 F1 0.1015\n",
      "val Loss: 1.0690 Acc: 0.1629 Percision: 0.2537 Recall 0.0827 F1 0.0982\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 1.0840 Acc: 0.1423 Percision: 0.2808 Recall 0.0926 F1 0.1214\n",
      "val Loss: 7.8067 Acc: 0.1849 Percision: 0.2205 Recall 0.0976 F1 0.1152\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 1.0598 Acc: 0.1582 Percision: 0.2963 Recall 0.1082 F1 0.1411\n",
      "val Loss: 4.1546 Acc: 0.1840 Percision: 0.2619 Recall 0.0986 F1 0.1088\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 0.9749 Acc: 0.1972 Percision: 0.3710 Recall 0.1309 F1 0.1669\n",
      "val Loss: 2.1366 Acc: 0.2342 Percision: 0.3179 Recall 0.1408 F1 0.1785\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 0.9437 Acc: 0.2190 Percision: 0.3639 Recall 0.1458 F1 0.1829\n",
      "val Loss: 16.2944 Acc: 0.2410 Percision: 0.2908 Recall 0.1483 F1 0.1826\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 0.9314 Acc: 0.2330 Percision: 0.4001 Recall 0.1537 F1 0.1929\n",
      "val Loss: 6.2687 Acc: 0.2461 Percision: 0.3551 Recall 0.1521 F1 0.1920\n",
      "\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 0.9191 Acc: 0.2330 Percision: 0.4053 Recall 0.1585 F1 0.1995\n",
      "val Loss: 0.9089 Acc: 0.2651 Percision: 0.3819 Recall 0.1677 F1 0.2096\n",
      "\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 0.9055 Acc: 0.2433 Percision: 0.4195 Recall 0.1677 F1 0.2112\n",
      "val Loss: 5.3758 Acc: 0.2582 Percision: 0.4508 Recall 0.1652 F1 0.2130\n",
      "\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 0.8981 Acc: 0.2487 Percision: 0.4278 Recall 0.1723 F1 0.2185\n",
      "val Loss: 0.8713 Acc: 0.2806 Percision: 0.4472 Recall 0.1839 F1 0.2245\n",
      "\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 0.8857 Acc: 0.2566 Percision: 0.4363 Recall 0.1819 F1 0.2288\n",
      "val Loss: 0.9110 Acc: 0.2622 Percision: 0.4661 Recall 0.1870 F1 0.2332\n",
      "\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 0.8768 Acc: 0.2607 Percision: 0.4387 Recall 0.1871 F1 0.2355\n",
      "val Loss: 0.8604 Acc: 0.2804 Percision: 0.4692 Recall 0.1897 F1 0.2353\n",
      "\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 0.8692 Acc: 0.2700 Percision: 0.4414 Recall 0.1918 F1 0.2420\n",
      "val Loss: 0.9479 Acc: 0.2817 Percision: 0.4528 Recall 0.1989 F1 0.2477\n",
      "\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 0.8588 Acc: 0.2724 Percision: 0.4352 Recall 0.1961 F1 0.2450\n",
      "val Loss: 0.9930 Acc: 0.2838 Percision: 0.5022 Recall 0.1886 F1 0.2382\n",
      "\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 0.8344 Acc: 0.2863 Percision: 0.4558 Recall 0.2099 F1 0.2631\n",
      "val Loss: 0.8195 Acc: 0.3053 Percision: 0.4962 Recall 0.2118 F1 0.2610\n",
      "\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 0.8287 Acc: 0.2924 Percision: 0.4656 Recall 0.2153 F1 0.2684\n",
      "val Loss: 1.1269 Acc: 0.3033 Percision: 0.4664 Recall 0.2058 F1 0.2549\n",
      "\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 0.8254 Acc: 0.2920 Percision: 0.4598 Recall 0.2175 F1 0.2696\n",
      "val Loss: 0.8165 Acc: 0.3069 Percision: 0.4900 Recall 0.2175 F1 0.2698\n",
      "\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 0.8233 Acc: 0.2920 Percision: 0.4846 Recall 0.2181 F1 0.2714\n",
      "val Loss: 0.8162 Acc: 0.3098 Percision: 0.5012 Recall 0.2139 F1 0.2646\n",
      "\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 0.8194 Acc: 0.2991 Percision: 0.4717 Recall 0.2189 F1 0.2724\n",
      "val Loss: 0.9572 Acc: 0.3115 Percision: 0.4651 Recall 0.2115 F1 0.2603\n",
      "\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 0.8218 Acc: 0.2989 Percision: 0.4533 Recall 0.2196 F1 0.2719\n",
      "val Loss: 1.0007 Acc: 0.3130 Percision: 0.4650 Recall 0.2168 F1 0.2688\n",
      "\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 0.8176 Acc: 0.2994 Percision: 0.5017 Recall 0.2229 F1 0.2774\n",
      "val Loss: 0.8158 Acc: 0.3104 Percision: 0.4826 Recall 0.2172 F1 0.2702\n",
      "\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 0.8159 Acc: 0.2990 Percision: 0.5032 Recall 0.2202 F1 0.2736\n",
      "val Loss: 0.8194 Acc: 0.3130 Percision: 0.4868 Recall 0.2201 F1 0.2716\n",
      "\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 0.8119 Acc: 0.3008 Percision: 0.4577 Recall 0.2218 F1 0.2745\n",
      "val Loss: 1.0552 Acc: 0.3123 Percision: 0.4859 Recall 0.2158 F1 0.2654\n",
      "\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 0.8110 Acc: 0.3025 Percision: 0.4645 Recall 0.2262 F1 0.2797\n",
      "val Loss: 2.0436 Acc: 0.3180 Percision: 0.4511 Recall 0.2211 F1 0.2712\n",
      "\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 0.8113 Acc: 0.3011 Percision: 0.5511 Recall 0.2234 F1 0.2769\n",
      "val Loss: 0.8169 Acc: 0.3125 Percision: 0.4552 Recall 0.2127 F1 0.2642\n",
      "\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 0.8092 Acc: 0.3026 Percision: 0.4698 Recall 0.2257 F1 0.2795\n",
      "val Loss: 0.8241 Acc: 0.3120 Percision: 0.4532 Recall 0.2168 F1 0.2680\n",
      "\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 0.8096 Acc: 0.3053 Percision: 0.4767 Recall 0.2241 F1 0.2770\n",
      "val Loss: 0.8890 Acc: 0.3168 Percision: 0.4539 Recall 0.2203 F1 0.2705\n",
      "\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 0.8083 Acc: 0.3024 Percision: 0.4673 Recall 0.2273 F1 0.2799\n",
      "val Loss: 0.8108 Acc: 0.3164 Percision: 0.4575 Recall 0.2172 F1 0.2687\n",
      "\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 0.8078 Acc: 0.3028 Percision: 0.4917 Recall 0.2236 F1 0.2764\n",
      "val Loss: 0.8118 Acc: 0.3120 Percision: 0.4584 Recall 0.2179 F1 0.2709\n",
      "\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 0.8054 Acc: 0.3056 Percision: 0.5038 Recall 0.2263 F1 0.2809\n",
      "val Loss: 1.0068 Acc: 0.3172 Percision: 0.4791 Recall 0.2227 F1 0.2739\n",
      "\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 0.8076 Acc: 0.3076 Percision: 0.5091 Recall 0.2260 F1 0.2801\n",
      "val Loss: 2.0920 Acc: 0.3180 Percision: 0.4483 Recall 0.2147 F1 0.2647\n",
      "\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 0.8062 Acc: 0.3074 Percision: 0.4829 Recall 0.2259 F1 0.2788\n",
      "val Loss: 1.6360 Acc: 0.3202 Percision: 0.4520 Recall 0.2198 F1 0.2713\n",
      "\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 0.8059 Acc: 0.3065 Percision: 0.5070 Recall 0.2280 F1 0.2820\n",
      "val Loss: 0.8116 Acc: 0.3148 Percision: 0.4559 Recall 0.2191 F1 0.2716\n",
      "\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 0.8052 Acc: 0.3046 Percision: 0.4613 Recall 0.2285 F1 0.2817\n",
      "val Loss: 1.5253 Acc: 0.3185 Percision: 0.4527 Recall 0.2193 F1 0.2712\n",
      "\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 0.8058 Acc: 0.3053 Percision: 0.4911 Recall 0.2274 F1 0.2812\n",
      "val Loss: 0.8097 Acc: 0.3156 Percision: 0.4561 Recall 0.2151 F1 0.2658\n",
      "\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 0.8079 Acc: 0.3053 Percision: 0.4548 Recall 0.2238 F1 0.2762\n",
      "val Loss: 0.9822 Acc: 0.3189 Percision: 0.4494 Recall 0.2211 F1 0.2713\n",
      "\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 0.8094 Acc: 0.3055 Percision: 0.5032 Recall 0.2268 F1 0.2800\n",
      "val Loss: 0.8108 Acc: 0.3165 Percision: 0.4598 Recall 0.2181 F1 0.2712\n",
      "\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 0.8027 Acc: 0.3063 Percision: 0.5198 Recall 0.2289 F1 0.2826\n",
      "val Loss: 0.8107 Acc: 0.3130 Percision: 0.4592 Recall 0.2147 F1 0.2673\n",
      "\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 0.8075 Acc: 0.3062 Percision: 0.4837 Recall 0.2284 F1 0.2828\n",
      "val Loss: 2.0046 Acc: 0.3185 Percision: 0.4538 Recall 0.2167 F1 0.2676\n",
      "\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 0.8061 Acc: 0.3075 Percision: 0.4781 Recall 0.2296 F1 0.2830\n",
      "val Loss: 1.3914 Acc: 0.3157 Percision: 0.4533 Recall 0.2165 F1 0.2674\n",
      "\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 0.8059 Acc: 0.3074 Percision: 0.5203 Recall 0.2289 F1 0.2836\n",
      "val Loss: 1.3897 Acc: 0.3131 Percision: 0.4523 Recall 0.2176 F1 0.2686\n",
      "\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 0.8067 Acc: 0.3028 Percision: 0.4626 Recall 0.2250 F1 0.2777\n",
      "val Loss: 0.8624 Acc: 0.3148 Percision: 0.4442 Recall 0.2200 F1 0.2686\n",
      "\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 0.8076 Acc: 0.3055 Percision: 0.4657 Recall 0.2272 F1 0.2807\n",
      "val Loss: 1.5019 Acc: 0.3148 Percision: 0.4506 Recall 0.2207 F1 0.2719\n",
      "\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 0.8047 Acc: 0.3073 Percision: 0.5333 Recall 0.2286 F1 0.2823\n",
      "val Loss: 1.5276 Acc: 0.3188 Percision: 0.4530 Recall 0.2174 F1 0.2671\n",
      "\n",
      "Training complete in 361m 6s\n"
     ]
    }
   ],
   "source": [
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "c6216757e2fd5f18f15c28d27f29427270f3789b"
   },
   "outputs": [],
   "source": [
    "# summary(model_ft, (4, IMG_SIZE, IMG_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomNet(\n",
       "  (custom_entry): CustomEntry(\n",
       "    (conv_1): Conv2d(4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  )\n",
       "  (backbone): RnetBackbone(\n",
       "    (backbone): Sequential(\n",
       "      (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): ReLU(inplace)\n",
       "      (2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (3): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (4): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (5): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (custom_head): CustomHead(\n",
       "    (flatten): Flatten()\n",
       "    (relu_1): ReLU()\n",
       "    (dropout_1): Dropout(p=0.5)\n",
       "    (fc_2): Linear(in_features=25088, out_features=256, bias=True)\n",
       "    (relu_2): ReLU()\n",
       "    (batchnorm_2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout_2): Dropout(p=0.5)\n",
       "    (fc_3): Linear(in_features=256, out_features=28, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
