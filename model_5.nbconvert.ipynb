{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, models\n",
    "\n",
    "from skimage import io\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches, patheffects\n",
    "\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "4248fd9c0afae8eaac391ab0d1895db2be249e75"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# base_path = r'../input'\n",
    "base_path = r'input'\n",
    "PATH_TRAIN_ANNO = os.path.join(base_path, 'train.csv')\n",
    "PATH_TRAIN_IMG = os.path.join(base_path, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "583ead53ccf420ab4290230a0a17cc3b6c62c74d"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from tensorboardX import SummaryWriter\n",
    "    USE_TENSORBOARD = True\n",
    "    writer = SummaryWriter()\n",
    "except:\n",
    "    USE_TENSORBOARD = False\n",
    "    print('No tensorboard X')\n",
    "\n",
    "def record_tb(phase, tag, value, global_step):\n",
    "    if USE_TENSORBOARD is True:\n",
    "        writer.add_scalar('{phase}/{tag}'.format(phase=phase, tag=tag), value, global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "7a95c899247b666f9235e3100a569744c46bf943"
   },
   "outputs": [],
   "source": [
    "# os.listdir(PATH_TRAIN_IMG)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "e85715fa2a9217474fc039c2b399c9b33490689a"
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 28\n",
    "MAX_TAGS = 5\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "VAL_SIZE =0.2\n",
    "THRESHOLD = 0.5\n",
    "SAMPLES = 1\n",
    "# DEVICE = torch.device(\"cpu\")\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_WORKERS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "7e81577b2c725960293a5cfabb29e0d46d66834c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample size: 31072\n",
      "[('00070df0-bbc3-11e8-b2bc-ac1f6b6435d0', [16, 0]),\n",
      " ('000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0', [7, 1, 2, 0]),\n",
      " ('000a9596-bbc4-11e8-b2bc-ac1f6b6435d0', [5])]\n"
     ]
    }
   ],
   "source": [
    "def get_transform_anno(annotation_path, img_path):\n",
    "    df = pd.read_csv(annotation_path)\n",
    "    annotations = []\n",
    "    for i, row in df.iterrows():\n",
    "        rcd_id = row['Id']\n",
    "        rcd_cate =  [int(j) for j in row['Target'].split()]\n",
    "        annotations.append((rcd_id, rcd_cate))\n",
    "    return annotations\n",
    "#get annotations\n",
    "annotations = get_transform_anno(PATH_TRAIN_ANNO, PATH_TRAIN_IMG)\n",
    "sample_size = int(len(annotations) * SAMPLES)\n",
    "print('sample size: {}'.format(sample_size))\n",
    "annotations = annotations[:sample_size]\n",
    "pprint(annotations[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "878d1a4d5d29884cc313258347df1ee630abae1a"
   },
   "outputs": [],
   "source": [
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, img_meta, img_path, transform = None):\n",
    "        self.img_meta = img_meta\n",
    "        self.transform = transform\n",
    "        self.channels = ['red', 'blue', 'yellow', 'green']\n",
    "        self.img_path = img_path\n",
    "        self.mlb = MultiLabelBinarizer(classes=range(0,NUM_CLASSES))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_meta)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id, img_tags= self.img_meta[idx]\n",
    "        ch = []\n",
    "        img_file_template = '{}_{}.png'\n",
    "        for c in self.channels:\n",
    "            ch.append(io.imread(os.path.join(self.img_path, img_file_template.format(img_id, c))))\n",
    "        img = np.stack(ch)\n",
    "\n",
    "        #augmentation\n",
    "        if bool(self.transform) is True:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        #binarize\n",
    "        img_tags = self.mlb.fit_transform([img_tags]).squeeze()\n",
    "        \n",
    "        #transform to tensor\n",
    "        img = torch.from_numpy(img).float()\n",
    "        img_tags = torch.from_numpy(img_tags)\n",
    "        \n",
    "        output = (img, img_tags)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "af583f0882e55fd31a3df230822ac86f99e25f4c"
   },
   "outputs": [],
   "source": [
    "class ImgTfm:\n",
    "    def __init__(self, aug_pipline = None):\n",
    "        self.seq = aug_pipline\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        \n",
    "#         seq_det = self.seq.to_deterministic()\n",
    "        \n",
    "        #augmentation\n",
    "        aug_img=img.copy().transpose((1, 2, 0))\n",
    "        aug_img = self.seq.augment_images([aug_img])[0]\n",
    "        aug_img=aug_img.transpose((2, 1, 0))\n",
    "        \n",
    "        #normalize\n",
    "        aug_img=aug_img/255\n",
    "        \n",
    "        return aug_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "7c0aeb64e2a7b651582a467d82079a2fda67c71b"
   },
   "outputs": [],
   "source": [
    "def get_aug_pipline(img_size, mode = 'train'):\n",
    "    if mode == 'train':\n",
    "        seq = iaa.Sequential([\n",
    "            iaa.Scale({\"height\": IMG_SIZE, \"width\": IMG_SIZE}),\n",
    "            iaa.Sequential([\n",
    "                iaa.Fliplr(0.5),\n",
    "                iaa.Affine(\n",
    "                    rotate=(-20, 20),\n",
    "                )\n",
    "            ], random_order=True) # apply augmenters in random order\n",
    "        ], random_order=False)\n",
    "    else: #ie.val\n",
    "        seq = iaa.Sequential([\n",
    "            iaa.Scale({\"height\": IMG_SIZE, \"width\": IMG_SIZE}),\n",
    "        ], random_order=False)\n",
    "#     seq = iaa.Sequential([\n",
    "#                 iaa.Scale({\"height\": IMG_SIZE, \"width\": IMG_SIZE}),\n",
    "#             ], random_order=False)\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "4c926ebe55fb37a7bfe3ffb2d6da2b145fb5f747"
   },
   "outputs": [],
   "source": [
    "train_set, val_set = train_test_split(annotations, test_size=VAL_SIZE, random_state=42)\n",
    "\n",
    "composed = {}\n",
    "composed['train'] = transforms.Compose([ImgTfm(aug_pipline=get_aug_pipline(img_size=IMG_SIZE, mode = 'train'))])\n",
    "composed['val'] = transforms.Compose([ImgTfm(aug_pipline=get_aug_pipline(img_size=IMG_SIZE, mode = 'val'))])\n",
    "\n",
    "image_datasets = {'train': ProteinDataset(train_set, img_path = PATH_TRAIN_IMG, transform=composed['train']),\n",
    "                 'val': ProteinDataset(val_set, img_path = PATH_TRAIN_IMG, transform=composed['val'])}\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, drop_last=True)\n",
    "              for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "e8c442abb721b3f65c4a7b076232757725311b2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 24857, 'val': 6215}\n"
     ]
    }
   ],
   "source": [
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "print(dataset_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "610b2e7a2e2b430673460bfd21e2940eca4a83b1"
   },
   "outputs": [],
   "source": [
    "#test dataset\n",
    "# ix = 10\n",
    "# tmp_img, tmp_tags  = image_datasets['train'][ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "116981e26006844aba80864a76434c9b0a445188"
   },
   "outputs": [],
   "source": [
    "#test dataloader\n",
    "# tmp_img, tmp_tags = next(iter(dataloaders['train']))\n",
    "# print('tmp_img shape: {}\\ntmp_tags: shape {}'.format(tmp_img.shape, tmp_tags.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "53926b18c61ec3d5f167ee905fcde5c0dc9289f5"
   },
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "    def forward(self, x): \n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "class RnetBackbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = self._prep_backbone()\n",
    "        \n",
    "    def _prep_backbone(self):     \n",
    "        base_model = models.resnet34(pretrained=False)\n",
    "        removed = list(base_model.children())[1:-2]\n",
    "        backbone = nn.Sequential(*removed)\n",
    "#         for param in backbone.parameters():\n",
    "#             param.require_grad = False\n",
    "        return backbone\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x\n",
    "\n",
    "class CustomHead(nn.Module):\n",
    "    def __init__(self, num_class):\n",
    "        super().__init__()\n",
    "        self.num_class = num_class\n",
    "        \n",
    "        self.flatten = Flatten()\n",
    "        self.relu_1 = nn.ReLU()\n",
    "        self.dropout_1 = nn.Dropout(p=0.5)\n",
    "        self.fc_2 = nn.Linear(512 * 7 * 7, 256)\n",
    "        self.relu_2 = nn.ReLU()\n",
    "        self.batchnorm_2 = nn.BatchNorm1d(256)\n",
    "        self.dropout_2 = nn.Dropout(p=0.5)\n",
    "        self.fc_3 = nn.Linear(256, self.num_class)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu_1(x)\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.fc_2(x)\n",
    "        x = self.relu_2(x)\n",
    "        x = self.batchnorm_2(x)\n",
    "        x = self.dropout_2(x)\n",
    "        x = self.fc_3(x)\n",
    "        return x\n",
    "\n",
    "class CustomEntry(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_1 = nn.Conv2d(in_channels=4, out_channels=64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_1(x)\n",
    "        return x\n",
    "    \n",
    "class CustomNet(nn.Module):\n",
    "    def __init__(self, num_class):\n",
    "        super().__init__()\n",
    "        self.custom_entry = CustomEntry()\n",
    "        self.backbone = RnetBackbone()\n",
    "        self.custom_head = CustomHead(num_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.custom_entry(x)\n",
    "        x = self.backbone(x)\n",
    "        x = self.custom_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "4ef191d711342bf7bce65d943f8629d03bcf26c1"
   },
   "outputs": [],
   "source": [
    "class F1Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        #f1 loss\n",
    "#         #prep y_true\n",
    "        y_true = y_true.float()\n",
    "\n",
    "        #prep y_pred\n",
    "        y_pred = torch.tensor(data = (torch.sigmoid(y_pred).ge(THRESHOLD)), dtype=torch.float, device=DEVICE, requires_grad=True)\n",
    "\n",
    "        #calculate loss\n",
    "        tp = (y_true * y_pred).sum(0).float()\n",
    "        # tn = ((1-y_true) * (1-y_pred)).sum(0).float()\n",
    "        fp = ((1-y_true) * y_pred).sum(0).float()\n",
    "        fn = (y_true * (1-y_pred)).sum(0).float()\n",
    "\n",
    "        p = tp / (tp + fp)\n",
    "        r = tp / (tp + fn)\n",
    "\n",
    "        f1 = 2*p*r / (p+r)\n",
    "        f1[torch.isnan(f1)] = 0\n",
    "        f1_loss = 1-f1.mean()\n",
    "#         print(f1_loss)\n",
    "        return f1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        target = target.float()\n",
    "        \n",
    "        if not (target.size() == input.size()):\n",
    "            raise ValueError(\"Target size ({}) must be the same as input size ({})\"\n",
    "                             .format(target.size(), input.size()))\n",
    "\n",
    "        max_val = (-input).clamp(min=0)\n",
    "        loss = input - input * target + max_val + \\\n",
    "            ((-max_val).exp() + (-input - max_val).exp()).log()\n",
    "\n",
    "        invprobs = F.logsigmoid(-input * (target * 2.0 - 1.0))\n",
    "        loss = (invprobs * self.gamma).exp() * loss\n",
    "        \n",
    "        return loss.sum(dim=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "34c786807f448f63f4244537f13ebca2a6b44ee2"
   },
   "outputs": [],
   "source": [
    "def prep_stats(y_pred, y_true):\n",
    "    #prep y_true\n",
    "    y_true_tfm = y_true.cpu().numpy().astype('uint8')\n",
    "    \n",
    "    #prep y_pred khot\n",
    "    y_pred_tfm = (torch.sigmoid(y_pred) > THRESHOLD).cpu().numpy().astype('uint8')\n",
    "    \n",
    "    return y_pred_tfm, y_true_tfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "7df1b550cc9567063a38437001eff118004b0370"
   },
   "outputs": [],
   "source": [
    "def calc_stats(y_pred, y_true, stats = 'accurancy'):\n",
    "    if stats == 'accuracy':\n",
    "        stat_value = accuracy_score(y_true, y_pred)\n",
    "    elif stats == 'precision':\n",
    "        stat_value = precision_score(y_true, y_pred, average = 'macro')\n",
    "    elif stats == 'recall':\n",
    "        stat_value = recall_score(y_true, y_pred, average = 'macro')\n",
    "    elif stats == 'f1':\n",
    "        stat_value = f1_score(y_true, y_pred, average = 'macro')\n",
    "    else:\n",
    "        stat_value = 0\n",
    "    return stat_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "467bc143b3e21ddb3674f77f2c78287c9bc3684c"
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=5):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_f1 = 0.0\n",
    "    steps = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            running_y_true = []\n",
    "            running_y_pred = []\n",
    "            \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, targets in dataloaders[phase]:\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                targets= targets.to(DEVICE)\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    \n",
    "                    y_pred_tfm, y_true_tfm = prep_stats(outputs, targets)\n",
    "                    running_y_pred.append(y_pred_tfm)\n",
    "                    running_y_true.append(y_true_tfm)\n",
    "                    \n",
    "                    #export step stats duing training phase\n",
    "                    if phase == 'train':\n",
    "                        record_tb(phase, 'loss', loss.cpu().data.numpy(), steps)\n",
    "                        record_tb(phase, 'accuracy', calc_stats(y_pred_tfm, y_true_tfm, stats = 'accurancy'), steps)\n",
    "                        record_tb(phase, 'precision', calc_stats(y_pred_tfm, y_true_tfm, stats = 'precision'), steps)\n",
    "                        record_tb(phase, 'recall', calc_stats(y_pred_tfm, y_true_tfm, stats = 'recall'), steps)\n",
    "                        record_tb(phase, 'f1', calc_stats(y_pred_tfm, y_true_tfm, stats = 'f1'), steps)\n",
    "                        steps += 1\n",
    "                        \n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            #calc epoch stats\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = accuracy_score(np.vstack(running_y_true), np.vstack(running_y_pred))\n",
    "            epoch_precision = precision_score(np.vstack(running_y_true), np.vstack(running_y_pred), average = 'macro')\n",
    "            epoch_recall = recall_score(np.vstack(running_y_true), np.vstack(running_y_pred), average = 'macro')\n",
    "            epoch_f1 = f1_score(np.vstack(running_y_true), np.vstack(running_y_pred), average = 'macro')\n",
    "            \n",
    "            #export epoch stats duing training phase\n",
    "            if phase == 'val':\n",
    "                record_tb(phase, 'loss', epoch_loss, steps)\n",
    "                record_tb(phase, 'accuracy', epoch_acc, steps)\n",
    "                record_tb(phase, 'precision', epoch_precision, steps)\n",
    "                record_tb(phase, 'recall', epoch_recall, steps)\n",
    "                record_tb(phase, 'f1', epoch_f1, steps)\n",
    "            \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f} Percision: {:.4f} Recall {:.4f} F1 {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc, epoch_precision, epoch_recall, epoch_f1))\n",
    "\n",
    "            # deep copy the model\n",
    "#             if phase == 'val' and epoch_acc > best_acc:\n",
    "#                 best_acc = epoch_acc\n",
    "#                 best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "#     print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "#     model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "be546ee708d7ccb01c9f876bb189db4818a7d12a"
   },
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_ft = CustomNet(num_class=NUM_CLASSES)\n",
    "model_ft = model_ft.to(DEVICE)\n",
    "\n",
    "criterion = FocalLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.01)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "a27461b7756ca489c66881be19cb1d0d54bf5d47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spacor/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/spacor/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/spacor/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/spacor/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.4045 Acc: 0.0306 Percision: 0.0777 Recall 0.0188 F1 0.0274\n",
      "val Loss: 1.2576 Acc: 0.0586 Percision: 0.0509 Recall 0.0280 F1 0.0312\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 1.2669 Acc: 0.0560 Percision: 0.1099 Recall 0.0332 F1 0.0388\n",
      "val Loss: 1.4328 Acc: 0.0496 Percision: 0.0944 Recall 0.0380 F1 0.0400\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 1.2426 Acc: 0.0658 Percision: 0.1221 Recall 0.0369 F1 0.0424\n",
      "val Loss: 1.3807 Acc: 0.0976 Percision: 0.1366 Recall 0.0457 F1 0.0478\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 1.2212 Acc: 0.0727 Percision: 0.1471 Recall 0.0405 F1 0.0482\n",
      "val Loss: 3.6479 Acc: 0.0462 Percision: 0.1283 Recall 0.0490 F1 0.0564\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 1.1875 Acc: 0.0841 Percision: 0.1732 Recall 0.0468 F1 0.0576\n",
      "val Loss: 1.6291 Acc: 0.0522 Percision: 0.1127 Recall 0.0255 F1 0.0379\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 1.2334 Acc: 0.0706 Percision: 0.2223 Recall 0.0392 F1 0.0520\n",
      "val Loss: 1.6517 Acc: 0.0781 Percision: 0.0685 Recall 0.0330 F1 0.0369\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 1.1886 Acc: 0.0908 Percision: 0.1876 Recall 0.0512 F1 0.0646\n",
      "val Loss: 3.8334 Acc: 0.1226 Percision: 0.1929 Recall 0.0794 F1 0.0888\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 1.1667 Acc: 0.0981 Percision: 0.1918 Recall 0.0562 F1 0.0721\n",
      "val Loss: 1.1085 Acc: 0.1261 Percision: 0.1437 Recall 0.0631 F1 0.0770\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 1.1467 Acc: 0.1087 Percision: 0.2490 Recall 0.0645 F1 0.0848\n",
      "val Loss: 24.3637 Acc: 0.1134 Percision: 0.2311 Recall 0.0650 F1 0.0835\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 1.1009 Acc: 0.1323 Percision: 0.2745 Recall 0.0837 F1 0.1109\n",
      "val Loss: 2.2778 Acc: 0.1888 Percision: 0.2672 Recall 0.1154 F1 0.1329\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 1.0184 Acc: 0.1720 Percision: 0.3362 Recall 0.1103 F1 0.1425\n",
      "val Loss: 1.4741 Acc: 0.2044 Percision: 0.2857 Recall 0.1174 F1 0.1476\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 0.9943 Acc: 0.1891 Percision: 0.3430 Recall 0.1207 F1 0.1547\n",
      "val Loss: 1.1086 Acc: 0.2183 Percision: 0.3092 Recall 0.1326 F1 0.1669\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 0.9769 Acc: 0.1994 Percision: 0.3784 Recall 0.1284 F1 0.1642\n",
      "val Loss: 1.8802 Acc: 0.2297 Percision: 0.2757 Recall 0.1369 F1 0.1660\n",
      "\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 0.9668 Acc: 0.2056 Percision: 0.3776 Recall 0.1342 F1 0.1704\n",
      "val Loss: 1.7255 Acc: 0.2437 Percision: 0.3544 Recall 0.1441 F1 0.1787\n",
      "\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 0.9505 Acc: 0.2174 Percision: 0.3922 Recall 0.1454 F1 0.1856\n",
      "val Loss: 2.0118 Acc: 0.2463 Percision: 0.3382 Recall 0.1534 F1 0.1921\n",
      "\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 0.9385 Acc: 0.2234 Percision: 0.4159 Recall 0.1540 F1 0.1962\n",
      "val Loss: 1.1867 Acc: 0.2600 Percision: 0.3606 Recall 0.1650 F1 0.2032\n",
      "\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 0.9283 Acc: 0.2315 Percision: 0.4137 Recall 0.1586 F1 0.2012\n",
      "val Loss: 1.6302 Acc: 0.2389 Percision: 0.3242 Recall 0.1533 F1 0.1933\n",
      "\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 0.9155 Acc: 0.2395 Percision: 0.4416 Recall 0.1678 F1 0.2137\n",
      "val Loss: 11.9017 Acc: 0.2737 Percision: 0.3759 Recall 0.1720 F1 0.2099\n",
      "\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 0.9100 Acc: 0.2454 Percision: 0.4236 Recall 0.1738 F1 0.2203\n",
      "val Loss: 0.8621 Acc: 0.2758 Percision: 0.4237 Recall 0.1810 F1 0.2260\n",
      "\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 0.8991 Acc: 0.2494 Percision: 0.4396 Recall 0.1764 F1 0.2240\n",
      "val Loss: 0.9227 Acc: 0.2411 Percision: 0.3898 Recall 0.1750 F1 0.2066\n",
      "\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 0.8779 Acc: 0.2652 Percision: 0.4451 Recall 0.1882 F1 0.2375\n",
      "val Loss: 2.7008 Acc: 0.2803 Percision: 0.4173 Recall 0.1858 F1 0.2312\n",
      "\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 0.8684 Acc: 0.2703 Percision: 0.4542 Recall 0.1919 F1 0.2411\n",
      "val Loss: 0.8589 Acc: 0.2853 Percision: 0.4300 Recall 0.1920 F1 0.2356\n",
      "\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 0.8644 Acc: 0.2734 Percision: 0.4694 Recall 0.1949 F1 0.2440\n",
      "val Loss: 0.8436 Acc: 0.2872 Percision: 0.4380 Recall 0.1935 F1 0.2348\n",
      "\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 0.8659 Acc: 0.2709 Percision: 0.4524 Recall 0.1957 F1 0.2446\n",
      "val Loss: 0.9331 Acc: 0.2858 Percision: 0.4395 Recall 0.1918 F1 0.2341\n",
      "\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 0.8633 Acc: 0.2745 Percision: 0.4955 Recall 0.1973 F1 0.2467\n",
      "val Loss: 1.0708 Acc: 0.2787 Percision: 0.4163 Recall 0.1945 F1 0.2364\n",
      "\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 0.8631 Acc: 0.2774 Percision: 0.4522 Recall 0.1979 F1 0.2479\n",
      "val Loss: 0.8545 Acc: 0.2808 Percision: 0.4186 Recall 0.1944 F1 0.2377\n",
      "\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 0.8605 Acc: 0.2784 Percision: 0.5314 Recall 0.1996 F1 0.2504\n",
      "val Loss: 1.6339 Acc: 0.2769 Percision: 0.4015 Recall 0.1949 F1 0.2362\n",
      "\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 0.8598 Acc: 0.2782 Percision: 0.4579 Recall 0.2016 F1 0.2523\n",
      "val Loss: 1.1426 Acc: 0.2906 Percision: 0.4210 Recall 0.1973 F1 0.2399\n",
      "\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 0.8570 Acc: 0.2792 Percision: 0.4633 Recall 0.2012 F1 0.2527\n",
      "val Loss: 1.9095 Acc: 0.2867 Percision: 0.4215 Recall 0.1988 F1 0.2389\n",
      "\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 0.8535 Acc: 0.2832 Percision: 0.4593 Recall 0.2037 F1 0.2549\n",
      "val Loss: 0.9174 Acc: 0.2901 Percision: 0.4633 Recall 0.1988 F1 0.2419\n",
      "\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 0.8521 Acc: 0.2840 Percision: 0.5072 Recall 0.2027 F1 0.2538\n",
      "val Loss: 0.8480 Acc: 0.2872 Percision: 0.4219 Recall 0.1985 F1 0.2415\n",
      "\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 0.8502 Acc: 0.2816 Percision: 0.4615 Recall 0.2055 F1 0.2573\n",
      "val Loss: 1.1250 Acc: 0.2858 Percision: 0.4224 Recall 0.2023 F1 0.2430\n",
      "\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 0.8510 Acc: 0.2832 Percision: 0.4564 Recall 0.2027 F1 0.2533\n",
      "val Loss: 0.8795 Acc: 0.2866 Percision: 0.4152 Recall 0.1973 F1 0.2381\n",
      "\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 0.8517 Acc: 0.2834 Percision: 0.4550 Recall 0.2031 F1 0.2538\n",
      "val Loss: 0.8516 Acc: 0.2851 Percision: 0.4151 Recall 0.2003 F1 0.2432\n",
      "\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 0.8520 Acc: 0.2811 Percision: 0.4876 Recall 0.2040 F1 0.2547\n",
      "val Loss: 0.8953 Acc: 0.2920 Percision: 0.4402 Recall 0.1982 F1 0.2426\n",
      "\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 0.8522 Acc: 0.2847 Percision: 0.4594 Recall 0.2065 F1 0.2582\n",
      "val Loss: 1.8579 Acc: 0.2908 Percision: 0.4126 Recall 0.2008 F1 0.2439\n",
      "\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 0.8514 Acc: 0.2804 Percision: 0.4715 Recall 0.2045 F1 0.2562\n",
      "val Loss: 1.1724 Acc: 0.2858 Percision: 0.4589 Recall 0.1981 F1 0.2420\n",
      "\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 0.8502 Acc: 0.2826 Percision: 0.4647 Recall 0.2043 F1 0.2554\n",
      "val Loss: 0.8740 Acc: 0.2835 Percision: 0.4564 Recall 0.1998 F1 0.2426\n",
      "\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 0.8511 Acc: 0.2849 Percision: 0.4632 Recall 0.2053 F1 0.2568\n",
      "val Loss: 0.8425 Acc: 0.2877 Percision: 0.4270 Recall 0.1989 F1 0.2435\n",
      "\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 0.8484 Acc: 0.2850 Percision: 0.4507 Recall 0.2039 F1 0.2539\n",
      "val Loss: 1.4377 Acc: 0.2819 Percision: 0.4425 Recall 0.1963 F1 0.2414\n",
      "\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 0.8483 Acc: 0.2865 Percision: 0.4637 Recall 0.2073 F1 0.2590\n",
      "val Loss: 1.0737 Acc: 0.2843 Percision: 0.4236 Recall 0.1977 F1 0.2416\n",
      "\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 0.8515 Acc: 0.2856 Percision: 0.5446 Recall 0.2059 F1 0.2580\n",
      "val Loss: 0.9201 Acc: 0.2928 Percision: 0.4399 Recall 0.2013 F1 0.2454\n",
      "\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 0.8503 Acc: 0.2860 Percision: 0.4639 Recall 0.2064 F1 0.2577\n",
      "val Loss: 2.2914 Acc: 0.2890 Percision: 0.4300 Recall 0.1957 F1 0.2399\n",
      "\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 0.8500 Acc: 0.2861 Percision: 0.4539 Recall 0.2057 F1 0.2561\n",
      "val Loss: 0.8491 Acc: 0.2893 Percision: 0.4559 Recall 0.2002 F1 0.2428\n",
      "\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 0.8499 Acc: 0.2848 Percision: 0.4903 Recall 0.2059 F1 0.2581\n",
      "val Loss: 1.8604 Acc: 0.2888 Percision: 0.4092 Recall 0.2004 F1 0.2415\n",
      "\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 0.8526 Acc: 0.2818 Percision: 0.4932 Recall 0.2051 F1 0.2568\n",
      "val Loss: 1.6166 Acc: 0.2941 Percision: 0.4345 Recall 0.2064 F1 0.2499\n",
      "\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 0.8506 Acc: 0.2829 Percision: 0.4565 Recall 0.2034 F1 0.2534\n",
      "val Loss: 0.8562 Acc: 0.2882 Percision: 0.4795 Recall 0.1973 F1 0.2403\n",
      "\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 0.8508 Acc: 0.2837 Percision: 0.4904 Recall 0.2052 F1 0.2562\n",
      "val Loss: 1.6912 Acc: 0.2814 Percision: 0.4508 Recall 0.1973 F1 0.2414\n",
      "\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 0.8491 Acc: 0.2864 Percision: 0.4593 Recall 0.2059 F1 0.2569\n",
      "val Loss: 1.5614 Acc: 0.2908 Percision: 0.4547 Recall 0.2027 F1 0.2438\n",
      "\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 0.8500 Acc: 0.2871 Percision: 0.4867 Recall 0.2055 F1 0.2553\n",
      "val Loss: 0.8495 Acc: 0.2893 Percision: 0.4437 Recall 0.1988 F1 0.2426\n",
      "\n",
      "Training complete in 360m 32s\n"
     ]
    }
   ],
   "source": [
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "c6216757e2fd5f18f15c28d27f29427270f3789b"
   },
   "outputs": [],
   "source": [
    "# summary(model_ft, (4, IMG_SIZE, IMG_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomNet(\n",
       "  (custom_entry): CustomEntry(\n",
       "    (conv_1): Conv2d(4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  )\n",
       "  (backbone): RnetBackbone(\n",
       "    (backbone): Sequential(\n",
       "      (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): ReLU(inplace)\n",
       "      (2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (3): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (4): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (5): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (custom_head): CustomHead(\n",
       "    (flatten): Flatten()\n",
       "    (relu_1): ReLU()\n",
       "    (dropout_1): Dropout(p=0.5)\n",
       "    (fc_2): Linear(in_features=25088, out_features=256, bias=True)\n",
       "    (relu_2): ReLU()\n",
       "    (batchnorm_2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout_2): Dropout(p=0.5)\n",
       "    (fc_3): Linear(in_features=256, out_features=28, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
