{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, models\n",
    "\n",
    "from skimage import io\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches, patheffects\n",
    "\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "c21a5bb776b463e9488f1928b636be7b5a92a1c2"
   },
   "outputs": [],
   "source": [
    "# from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# base_path = r'../input'\n",
    "base_path = r'input'\n",
    "PATH_TRAIN_ANNO = os.path.join(base_path, 'train.csv')\n",
    "PATH_TRAIN_IMG = os.path.join(base_path, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "583ead53ccf420ab4290230a0a17cc3b6c62c74d"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from tensorboardX import SummaryWriter\n",
    "    USE_TENSORBOARD = True\n",
    "    writer = SummaryWriter()\n",
    "except:\n",
    "    USE_TENSORBOARD = False\n",
    "    print('No tensorboard X')\n",
    "\n",
    "def record_tb(phase, tag, value, global_step):\n",
    "    if USE_TENSORBOARD is True:\n",
    "        writer.add_scalar('{phase}/{tag}'.format(phase=phase, tag=tag), value, global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "7a95c899247b666f9235e3100a569744c46bf943"
   },
   "outputs": [],
   "source": [
    "# os.listdir(PATH_TRAIN_IMG)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "e85715fa2a9217474fc039c2b399c9b33490689a"
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 28\n",
    "MAX_TAGS = 5\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "VAL_SIZE =0.2\n",
    "THRESHOLD = 0.5\n",
    "SAMPLES = 1\n",
    "base_lr = 0.01\n",
    "# DEVICE = torch.device(\"cpu\")\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_WORKERS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "7e81577b2c725960293a5cfabb29e0d46d66834c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample size: 31072\n",
      "[('00070df0-bbc3-11e8-b2bc-ac1f6b6435d0', [16, 0]),\n",
      " ('000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0', [7, 1, 2, 0]),\n",
      " ('000a9596-bbc4-11e8-b2bc-ac1f6b6435d0', [5])]\n"
     ]
    }
   ],
   "source": [
    "def get_transform_anno(annotation_path, img_path):\n",
    "    df = pd.read_csv(annotation_path)\n",
    "    annotations = []\n",
    "    for i, row in df.iterrows():\n",
    "        rcd_id = row['Id']\n",
    "        rcd_cate =  [int(j) for j in row['Target'].split()]\n",
    "        annotations.append((rcd_id, rcd_cate))\n",
    "    return annotations\n",
    "#get annotations\n",
    "annotations = get_transform_anno(PATH_TRAIN_ANNO, PATH_TRAIN_IMG)\n",
    "sample_size = int(len(annotations) * SAMPLES)\n",
    "print('sample size: {}'.format(sample_size))\n",
    "annotations = annotations[:sample_size]\n",
    "pprint(annotations[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "878d1a4d5d29884cc313258347df1ee630abae1a"
   },
   "outputs": [],
   "source": [
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, img_meta, img_path, transform = None):\n",
    "        self.img_meta = img_meta\n",
    "        self.transform = transform\n",
    "        self.channels = ['red', 'blue', 'yellow', 'green']\n",
    "        self.img_path = img_path\n",
    "        self.mlb = MultiLabelBinarizer(classes=range(0,NUM_CLASSES))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_meta)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id, img_tags= self.img_meta[idx]\n",
    "        ch = []\n",
    "        img_file_template = '{}_{}.png'\n",
    "        for c in self.channels:\n",
    "            ch.append(io.imread(os.path.join(self.img_path, img_file_template.format(img_id, c))))\n",
    "        img = np.stack(ch)\n",
    "\n",
    "        #augmentation\n",
    "        if bool(self.transform) is True:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        #binarize\n",
    "        img_tags = self.mlb.fit_transform([img_tags]).squeeze()\n",
    "        \n",
    "        #transform to tensor\n",
    "        img = torch.from_numpy(img).float()\n",
    "        img_tags = torch.from_numpy(img_tags)\n",
    "        \n",
    "        output = (img, img_tags)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "af583f0882e55fd31a3df230822ac86f99e25f4c"
   },
   "outputs": [],
   "source": [
    "class ImgTfm:\n",
    "    def __init__(self, aug_pipline = None):\n",
    "        self.seq = aug_pipline\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        \n",
    "#         seq_det = self.seq.to_deterministic()\n",
    "        \n",
    "        #augmentation\n",
    "        aug_img=img.copy().transpose((1, 2, 0))\n",
    "        aug_img = self.seq.augment_images([aug_img])[0]\n",
    "        aug_img=aug_img.transpose((2, 1, 0))\n",
    "        \n",
    "        #normalize\n",
    "        aug_img=aug_img/255\n",
    "        \n",
    "        return aug_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "7c0aeb64e2a7b651582a467d82079a2fda67c71b"
   },
   "outputs": [],
   "source": [
    "def get_aug_pipline(img_size, mode = 'train'):\n",
    "    if mode == 'train':\n",
    "        seq = iaa.Sequential([\n",
    "            iaa.Scale({\"height\": IMG_SIZE, \"width\": IMG_SIZE}),\n",
    "            iaa.Sequential([\n",
    "                iaa.Fliplr(0.5),\n",
    "                iaa.Affine(\n",
    "                    rotate=(-20, 20),\n",
    "                )\n",
    "            ], random_order=True) # apply augmenters in random order\n",
    "        ], random_order=False)\n",
    "    else: #ie.val\n",
    "        seq = iaa.Sequential([\n",
    "            iaa.Scale({\"height\": IMG_SIZE, \"width\": IMG_SIZE}),\n",
    "        ], random_order=False)\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "4c926ebe55fb37a7bfe3ffb2d6da2b145fb5f747"
   },
   "outputs": [],
   "source": [
    "train_set, val_set = train_test_split(annotations, test_size=VAL_SIZE, random_state=42)\n",
    "\n",
    "composed = {}\n",
    "composed['train'] = transforms.Compose([ImgTfm(aug_pipline=get_aug_pipline(img_size=IMG_SIZE, mode = 'train'))])\n",
    "composed['val'] = transforms.Compose([ImgTfm(aug_pipline=get_aug_pipline(img_size=IMG_SIZE, mode = 'val'))])\n",
    "\n",
    "image_datasets = {'train': ProteinDataset(train_set, img_path = PATH_TRAIN_IMG, transform=composed['train']),\n",
    "                 'val': ProteinDataset(val_set, img_path = PATH_TRAIN_IMG, transform=composed['val'])}\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, drop_last=True)\n",
    "              for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "e8c442abb721b3f65c4a7b076232757725311b2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 24857, 'val': 6215}\n"
     ]
    }
   ],
   "source": [
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "print(dataset_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "610b2e7a2e2b430673460bfd21e2940eca4a83b1"
   },
   "outputs": [],
   "source": [
    "#test dataset\n",
    "# ix = 10\n",
    "# tmp_img, tmp_tags  = image_datasets['train'][ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "116981e26006844aba80864a76434c9b0a445188"
   },
   "outputs": [],
   "source": [
    "#test dataloader\n",
    "# tmp_img, tmp_tags = next(iter(dataloaders['train']))\n",
    "# print('tmp_img shape: {}\\ntmp_tags: shape {}'.format(tmp_img.shape, tmp_tags.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "53926b18c61ec3d5f167ee905fcde5c0dc9289f5"
   },
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "    def forward(self, x): \n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "class RnetBackbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = self._prep_backbone()\n",
    "        \n",
    "    def _prep_backbone(self):     \n",
    "        base_model = models.resnet34(pretrained=True)\n",
    "        removed = list(base_model.children())[1:-2]\n",
    "        backbone = nn.Sequential(*removed)\n",
    "#         for param in backbone.parameters():\n",
    "#             param.require_grad = False\n",
    "        return backbone\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x\n",
    "\n",
    "class CustomHead(nn.Module):\n",
    "    def __init__(self, num_class):\n",
    "        super().__init__()\n",
    "        self.num_class = num_class\n",
    "        \n",
    "        self.flatten = Flatten()\n",
    "        self.relu_1 = nn.ReLU()\n",
    "        self.dropout_1 = nn.Dropout(p=0.5)\n",
    "        self.fc_2 = nn.Linear(512 * 7 * 7, 256)\n",
    "        self.relu_2 = nn.ReLU()\n",
    "        self.batchnorm_2 = nn.BatchNorm1d(256)\n",
    "        self.dropout_2 = nn.Dropout(p=0.5)\n",
    "        self.fc_3 = nn.Linear(256, self.num_class)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu_1(x)\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.fc_2(x)\n",
    "        x = self.relu_2(x)\n",
    "        x = self.batchnorm_2(x)\n",
    "        x = self.dropout_2(x)\n",
    "        x = self.fc_3(x)\n",
    "        return x\n",
    "\n",
    "# class CustomEntry(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.conv_1 = nn.Conv2d(in_channels=4, out_channels=64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "#         nn.init.kaiming_normal_(self.conv_1.weight, mode='fan_out', nonlinearity='relu')\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv_1(x)\n",
    "#         return x\n",
    "\n",
    "class CustomEntry(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_1 = self._prep_layers()\n",
    "        \n",
    "    def _prep_layers(self):\n",
    "        model = models.resnet34(pretrained=True)\n",
    "        original_entry_w = torch.tensor(list(model.children())[0].weight)\n",
    "        new_entry_w = torch.cat([original_entry_w, torch.zeros(size = (64,1,7,7))], 1)\n",
    "        \n",
    "        conv_1 = nn.Conv2d(in_channels=4, out_channels=64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        conv_1.weight=conv_1.weight = torch.nn.Parameter(new_entry_w)\n",
    "        return conv_1\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_1(x)\n",
    "        return x\n",
    "    \n",
    "class CustomNet(nn.Module):\n",
    "    def __init__(self, num_class):\n",
    "        super().__init__()\n",
    "        self.custom_entry = CustomEntry()\n",
    "        self.backbone = RnetBackbone()\n",
    "        self.custom_head = CustomHead(num_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.custom_entry(x)\n",
    "        x = self.backbone(x)\n",
    "        x = self.custom_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "4ef191d711342bf7bce65d943f8629d03bcf26c1"
   },
   "outputs": [],
   "source": [
    "class F1Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        #f1 loss\n",
    "#         #prep y_true\n",
    "        y_true = y_true.float()\n",
    "\n",
    "        #prep y_pred\n",
    "        y_pred = torch.tensor(data = (torch.sigmoid(y_pred).ge(THRESHOLD)), dtype=torch.float, device=DEVICE, requires_grad=True)\n",
    "\n",
    "        #calculate loss\n",
    "        tp = (y_true * y_pred).sum(0).float()\n",
    "        # tn = ((1-y_true) * (1-y_pred)).sum(0).float()\n",
    "        fp = ((1-y_true) * y_pred).sum(0).float()\n",
    "        fn = (y_true * (1-y_pred)).sum(0).float()\n",
    "\n",
    "        p = tp / (tp + fp)\n",
    "        r = tp / (tp + fn)\n",
    "\n",
    "        f1 = 2*p*r / (p+r)\n",
    "        f1[torch.isnan(f1)] = 0\n",
    "        f1_loss = 1-f1.mean()\n",
    "#         print(f1_loss)\n",
    "        return f1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "cc00e0b447f5877e1c8f2c669a0776213c480892"
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        target = target.float()\n",
    "        \n",
    "        if not (target.size() == input.size()):\n",
    "            raise ValueError(\"Target size ({}) must be the same as input size ({})\"\n",
    "                             .format(target.size(), input.size()))\n",
    "\n",
    "        max_val = (-input).clamp(min=0)\n",
    "        loss = input - input * target + max_val + \\\n",
    "            ((-max_val).exp() + (-input - max_val).exp()).log()\n",
    "\n",
    "        invprobs = F.logsigmoid(-input * (target * 2.0 - 1.0))\n",
    "        loss = (invprobs * self.gamma).exp() * loss\n",
    "        \n",
    "        return loss.sum(dim=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "34c786807f448f63f4244537f13ebca2a6b44ee2"
   },
   "outputs": [],
   "source": [
    "def prep_stats(y_pred, y_true):\n",
    "    #prep y_true\n",
    "    y_true_tfm = y_true.cpu().numpy().astype('uint8')\n",
    "    \n",
    "    #prep y_pred khot\n",
    "    y_pred_tfm = (torch.sigmoid(y_pred) > THRESHOLD).cpu().numpy().astype('uint8')\n",
    "    \n",
    "    return y_pred_tfm, y_true_tfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "7df1b550cc9567063a38437001eff118004b0370"
   },
   "outputs": [],
   "source": [
    "def calc_stats(y_pred, y_true, stats = 'accurancy'):\n",
    "    if stats == 'accuracy':\n",
    "        stat_value = accuracy_score(y_true, y_pred)\n",
    "    elif stats == 'precision':\n",
    "        stat_value = precision_score(y_true, y_pred, average = 'macro')\n",
    "    elif stats == 'recall':\n",
    "        stat_value = recall_score(y_true, y_pred, average = 'macro')\n",
    "    elif stats == 'f1':\n",
    "        stat_value = f1_score(y_true, y_pred, average = 'macro')\n",
    "    else:\n",
    "        stat_value = 0\n",
    "    return stat_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "467bc143b3e21ddb3674f77f2c78287c9bc3684c"
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=5, init_steps = 0):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_f1 = 0.0\n",
    "    steps = init_steps\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            running_y_true = []\n",
    "            running_y_pred = []\n",
    "            \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, targets in dataloaders[phase]:\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                targets= targets.to(DEVICE)\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    \n",
    "                    y_pred_tfm, y_true_tfm = prep_stats(outputs, targets)\n",
    "                    running_y_pred.append(y_pred_tfm)\n",
    "                    running_y_true.append(y_true_tfm)\n",
    "                    \n",
    "                    #export step stats duing training phase\n",
    "                    if phase == 'train':\n",
    "                        record_tb(phase, 'loss', loss.cpu().data.numpy(), steps)\n",
    "                        record_tb(phase, 'accuracy', calc_stats(y_pred_tfm, y_true_tfm, stats = 'accurancy'), steps)\n",
    "                        record_tb(phase, 'precision', calc_stats(y_pred_tfm, y_true_tfm, stats = 'precision'), steps)\n",
    "                        record_tb(phase, 'recall', calc_stats(y_pred_tfm, y_true_tfm, stats = 'recall'), steps)\n",
    "                        record_tb(phase, 'f1', calc_stats(y_pred_tfm, y_true_tfm, stats = 'f1'), steps)\n",
    "                        steps += 1\n",
    "                        \n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            #calc epoch stats\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = accuracy_score(np.vstack(running_y_true), np.vstack(running_y_pred))\n",
    "            epoch_precision = precision_score(np.vstack(running_y_true), np.vstack(running_y_pred), average = 'macro')\n",
    "            epoch_recall = recall_score(np.vstack(running_y_true), np.vstack(running_y_pred), average = 'macro')\n",
    "            epoch_f1 = f1_score(np.vstack(running_y_true), np.vstack(running_y_pred), average = 'macro')\n",
    "            \n",
    "            #export epoch stats duing training phase\n",
    "            if phase == 'val':\n",
    "                record_tb(phase, 'loss', epoch_loss, steps)\n",
    "                record_tb(phase, 'accuracy', epoch_acc, steps)\n",
    "                record_tb(phase, 'precision', epoch_precision, steps)\n",
    "                record_tb(phase, 'recall', epoch_recall, steps)\n",
    "                record_tb(phase, 'f1', epoch_f1, steps)\n",
    "            \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f} Percision: {:.4f} Recall {:.4f} F1 {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc, epoch_precision, epoch_recall, epoch_f1))\n",
    "\n",
    "            # deep copy the model\n",
    "#             if phase == 'val' and epoch_acc > best_acc:\n",
    "#                 best_acc = epoch_acc\n",
    "#                 best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "#     print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "#     model.load_state_dict(best_model_wts)\n",
    "    return model, steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "2f201122c590638f86357241d2df7204e9712213"
   },
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_ft = CustomNet(num_class=NUM_CLASSES)\n",
    "model_ft = model_ft.to(DEVICE)\n",
    "\n",
    "criterion = FocalLoss()\n",
    "\n",
    "steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "57023ae1b0a357229aca85009dd7d38eb574e500"
   },
   "outputs": [],
   "source": [
    "# # train \n",
    "# # Observe that all parameters are being optimized\n",
    "# optimizer_ft = optim.Adam(model_ft.parameters(), lr=base_lr)\n",
    "\n",
    "# # Decay LR by a factor of 0.1 every 10 epochs\n",
    "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.1)\n",
    "\n",
    "# #Freeze backbone\n",
    "# for param in model_ft.backbone.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# model_ft, steps = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "#                        num_epochs=20, init_steps=steps)\n",
    "\n",
    "# #save intermediate model\n",
    "# torch.save(model_ft.state_dict(), 'M17_20181106_stage_1.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "da3c4971f0cfef2943b41c6db649ca4127ad1453"
   },
   "outputs": [],
   "source": [
    "#load_weight\n",
    "model_w_path = r'M17_real_20181106_cycle_stage_2.model'\n",
    "model_ft.load_state_dict(torch.load(model_w_path))\n",
    "\n",
    "#Unfreeze everything\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Different learning rate for different layers\n",
    "optimizer_ft = optim.Adam([\n",
    "    {'params': model_ft.custom_entry.parameters(), 'lr': base_lr/3},\n",
    "    {'params': model_ft.backbone.parameters(), 'lr': base_lr/10},\n",
    "    {'params': model_ft.custom_head.parameters()},\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "06eda9ec601b47a0d7d32e5afc8317f16b6ddd7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spacor/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/spacor/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/spacor/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/spacor/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.7345 Acc: 0.3715 Percision: 0.5876 Recall 0.3316 F1 0.4032\n",
      "val Loss: 0.7751 Acc: 0.3615 Percision: 0.6067 Recall 0.3201 F1 0.3905\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 0.7256 Acc: 0.3777 Percision: 0.6037 Recall 0.3416 F1 0.4158\n",
      "val Loss: 0.8236 Acc: 0.3331 Percision: 0.5332 Recall 0.2804 F1 0.3308\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 0.7257 Acc: 0.3748 Percision: 0.6227 Recall 0.3469 F1 0.4243\n",
      "val Loss: 0.8616 Acc: 0.3257 Percision: 0.5627 Recall 0.2977 F1 0.3536\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 0.7187 Acc: 0.3825 Percision: 0.6480 Recall 0.3594 F1 0.4378\n",
      "val Loss: 0.8257 Acc: 0.3400 Percision: 0.5055 Recall 0.3079 F1 0.3550\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 0.7144 Acc: 0.3835 Percision: 0.5909 Recall 0.3500 F1 0.4231\n",
      "val Loss: 0.8105 Acc: 0.3591 Percision: 0.6341 Recall 0.3250 F1 0.3865\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 0.7029 Acc: 0.3852 Percision: 0.6483 Recall 0.3821 F1 0.4624\n",
      "val Loss: 0.8149 Acc: 0.3533 Percision: 0.6260 Recall 0.2971 F1 0.3723\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 0.7011 Acc: 0.3917 Percision: 0.6453 Recall 0.3653 F1 0.4427\n",
      "val Loss: 0.7718 Acc: 0.3724 Percision: 0.5889 Recall 0.3518 F1 0.4088\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 0.6933 Acc: 0.3919 Percision: 0.6517 Recall 0.3808 F1 0.4623\n",
      "val Loss: 0.8001 Acc: 0.3752 Percision: 0.6360 Recall 0.3127 F1 0.3787\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 0.6906 Acc: 0.3942 Percision: 0.6380 Recall 0.3810 F1 0.4600\n",
      "val Loss: 0.8050 Acc: 0.3763 Percision: 0.5732 Recall 0.3163 F1 0.3690\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 0.6849 Acc: 0.3959 Percision: 0.6407 Recall 0.3905 F1 0.4688\n",
      "val Loss: 0.7690 Acc: 0.3798 Percision: 0.5998 Recall 0.3474 F1 0.4125\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 0.5943 Acc: 0.4519 Percision: 0.7283 Recall 0.4528 F1 0.5413\n",
      "val Loss: 0.7203 Acc: 0.4188 Percision: 0.7152 Recall 0.4005 F1 0.4840\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 0.5638 Acc: 0.4719 Percision: 0.7574 Recall 0.4866 F1 0.5736\n",
      "val Loss: 0.7354 Acc: 0.4129 Percision: 0.6657 Recall 0.3933 F1 0.4709\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 0.5614 Acc: 0.4781 Percision: 0.7672 Recall 0.4981 F1 0.5869\n",
      "val Loss: 0.7218 Acc: 0.4217 Percision: 0.7096 Recall 0.4065 F1 0.4914\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 0.5517 Acc: 0.4839 Percision: 0.7733 Recall 0.5039 F1 0.5919\n",
      "val Loss: 0.7232 Acc: 0.4183 Percision: 0.7138 Recall 0.3981 F1 0.4790\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 0.5434 Acc: 0.4903 Percision: 0.7601 Recall 0.5071 F1 0.5940\n",
      "val Loss: 0.7231 Acc: 0.4148 Percision: 0.7043 Recall 0.3868 F1 0.4700\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.5364 Acc: 0.4944 Percision: 0.7664 Recall 0.5161 F1 0.6015\n",
      "val Loss: 0.7300 Acc: 0.4199 Percision: 0.7195 Recall 0.3965 F1 0.4802\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.5315 Acc: 0.4964 Percision: 0.7469 Recall 0.5164 F1 0.5975\n",
      "val Loss: 1.0117 Acc: 0.4195 Percision: 0.6723 Recall 0.3975 F1 0.4730\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.5258 Acc: 0.5023 Percision: 0.7716 Recall 0.5185 F1 0.6021\n",
      "val Loss: 0.7324 Acc: 0.4224 Percision: 0.7496 Recall 0.4078 F1 0.4986\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.5244 Acc: 0.5000 Percision: 0.7626 Recall 0.5251 F1 0.6094\n",
      "val Loss: 0.7578 Acc: 0.4145 Percision: 0.7374 Recall 0.3976 F1 0.4857\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.5232 Acc: 0.5029 Percision: 0.7583 Recall 0.5183 F1 0.6014\n",
      "val Loss: 0.7402 Acc: 0.4196 Percision: 0.6929 Recall 0.4136 F1 0.4925\n",
      "\n",
      "Training complete in 137m 3s\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 0.6425 Acc: 0.4234 Percision: 0.6821 Recall 0.4205 F1 0.5026\n",
      "val Loss: 0.8090 Acc: 0.3629 Percision: 0.6706 Recall 0.3285 F1 0.4067\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 0.6467 Acc: 0.4200 Percision: 0.6640 Recall 0.4319 F1 0.5124\n",
      "val Loss: 0.8953 Acc: 0.3484 Percision: 0.6406 Recall 0.3286 F1 0.3831\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 0.6379 Acc: 0.4247 Percision: 0.6813 Recall 0.4454 F1 0.5262\n",
      "val Loss: 0.7860 Acc: 0.3853 Percision: 0.6610 Recall 0.3737 F1 0.4478\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 0.6390 Acc: 0.4267 Percision: 0.6958 Recall 0.4429 F1 0.5280\n",
      "val Loss: 0.8225 Acc: 0.3732 Percision: 0.5905 Recall 0.3673 F1 0.4249\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 0.6278 Acc: 0.4338 Percision: 0.6646 Recall 0.4403 F1 0.5187\n",
      "val Loss: 0.8174 Acc: 0.3831 Percision: 0.6210 Recall 0.3589 F1 0.4211\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 0.6300 Acc: 0.4329 Percision: 0.7052 Recall 0.4472 F1 0.5292\n",
      "val Loss: 0.7742 Acc: 0.3851 Percision: 0.6486 Recall 0.3327 F1 0.4072\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 0.6133 Acc: 0.4419 Percision: 0.6930 Recall 0.4688 F1 0.5484\n",
      "val Loss: 0.7753 Acc: 0.3940 Percision: 0.6816 Recall 0.3374 F1 0.4079\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 0.6149 Acc: 0.4419 Percision: 0.6700 Recall 0.4511 F1 0.5286\n",
      "val Loss: 0.9140 Acc: 0.3811 Percision: 0.6844 Recall 0.3677 F1 0.4442\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 0.6066 Acc: 0.4457 Percision: 0.7036 Recall 0.4685 F1 0.5515\n",
      "val Loss: 0.8203 Acc: 0.3700 Percision: 0.6607 Recall 0.3794 F1 0.4507\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 0.5992 Acc: 0.4509 Percision: 0.7221 Recall 0.4737 F1 0.5571\n",
      "val Loss: 0.8328 Acc: 0.3972 Percision: 0.7071 Recall 0.3494 F1 0.4311\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 0.5091 Acc: 0.5145 Percision: 0.7818 Recall 0.5545 F1 0.6387\n",
      "val Loss: 0.7486 Acc: 0.4236 Percision: 0.6895 Recall 0.4191 F1 0.5032\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 0.4743 Acc: 0.5342 Percision: 0.7894 Recall 0.5914 F1 0.6671\n",
      "val Loss: 0.7668 Acc: 0.4280 Percision: 0.6691 Recall 0.4198 F1 0.4958\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 0.4621 Acc: 0.5444 Percision: 0.8068 Recall 0.6127 F1 0.6882\n",
      "val Loss: 0.7675 Acc: 0.4349 Percision: 0.6930 Recall 0.4249 F1 0.5069\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 0.4567 Acc: 0.5495 Percision: 0.8008 Recall 0.6004 F1 0.6731\n",
      "val Loss: 0.7716 Acc: 0.4327 Percision: 0.6603 Recall 0.4346 F1 0.5102\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 0.4467 Acc: 0.5553 Percision: 0.7982 Recall 0.6071 F1 0.6776\n",
      "val Loss: 0.7811 Acc: 0.4341 Percision: 0.6862 Recall 0.4291 F1 0.5099\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.4421 Acc: 0.5577 Percision: 0.8046 Recall 0.6192 F1 0.6913\n",
      "val Loss: 0.8001 Acc: 0.4277 Percision: 0.6741 Recall 0.4198 F1 0.5009\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.4387 Acc: 0.5675 Percision: 0.8261 Recall 0.6257 F1 0.7035\n",
      "val Loss: 0.7937 Acc: 0.4328 Percision: 0.6569 Recall 0.4286 F1 0.5031\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.4336 Acc: 0.5662 Percision: 0.8243 Recall 0.6291 F1 0.7018\n",
      "val Loss: 0.8179 Acc: 0.4340 Percision: 0.6620 Recall 0.4139 F1 0.4932\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.4275 Acc: 0.5716 Percision: 0.8335 Recall 0.6335 F1 0.7075\n",
      "val Loss: 0.7952 Acc: 0.4373 Percision: 0.6801 Recall 0.4576 F1 0.5356\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.4240 Acc: 0.5728 Percision: 0.7970 Recall 0.6198 F1 0.6909\n",
      "val Loss: 0.8002 Acc: 0.4327 Percision: 0.7063 Recall 0.4502 F1 0.5327\n",
      "\n",
      "Training complete in 137m 13s\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 0.5560 Acc: 0.4834 Percision: 0.7161 Recall 0.5048 F1 0.5822\n",
      "val Loss: 0.8519 Acc: 0.3913 Percision: 0.6502 Recall 0.3737 F1 0.4416\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 0.5574 Acc: 0.4814 Percision: 0.7572 Recall 0.5263 F1 0.6080\n",
      "val Loss: 0.8389 Acc: 0.4003 Percision: 0.7153 Recall 0.3755 F1 0.4465\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 0.5616 Acc: 0.4776 Percision: 0.7194 Recall 0.5188 F1 0.5929\n",
      "val Loss: 0.8103 Acc: 0.4009 Percision: 0.7058 Recall 0.3728 F1 0.4398\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 0.5553 Acc: 0.4843 Percision: 0.7365 Recall 0.5317 F1 0.6099\n",
      "val Loss: 0.8519 Acc: 0.4027 Percision: 0.6504 Recall 0.3435 F1 0.4111\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 0.5493 Acc: 0.4853 Percision: 0.7528 Recall 0.5337 F1 0.6125\n",
      "val Loss: 0.9191 Acc: 0.3649 Percision: 0.6453 Recall 0.3450 F1 0.4147\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 0.5442 Acc: 0.4894 Percision: 0.7006 Recall 0.5200 F1 0.5900\n",
      "val Loss: 0.8913 Acc: 0.3708 Percision: 0.7107 Recall 0.4045 F1 0.4791\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 0.5329 Acc: 0.4940 Percision: 0.7307 Recall 0.5499 F1 0.6211\n",
      "val Loss: 0.8770 Acc: 0.4014 Percision: 0.6313 Recall 0.3908 F1 0.4472\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 0.5340 Acc: 0.4959 Percision: 0.7450 Recall 0.5442 F1 0.6208\n",
      "val Loss: 0.8483 Acc: 0.3882 Percision: 0.5984 Recall 0.3801 F1 0.4389\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 0.5229 Acc: 0.5023 Percision: 0.7470 Recall 0.5510 F1 0.6276\n",
      "val Loss: 1.8676 Acc: 0.3919 Percision: 0.6516 Recall 0.3549 F1 0.4422\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 0.5256 Acc: 0.5025 Percision: 0.7576 Recall 0.5679 F1 0.6415\n",
      "val Loss: 0.9099 Acc: 0.3971 Percision: 0.6812 Recall 0.3505 F1 0.4246\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 0.4250 Acc: 0.5748 Percision: 0.8327 Recall 0.6253 F1 0.7039\n",
      "val Loss: 0.8196 Acc: 0.4335 Percision: 0.7174 Recall 0.4598 F1 0.5419\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 0.3906 Acc: 0.5992 Percision: 0.8451 Recall 0.6799 F1 0.7489\n",
      "val Loss: 0.8417 Acc: 0.4370 Percision: 0.7217 Recall 0.4534 F1 0.5381\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 0.3768 Acc: 0.6132 Percision: 0.8429 Recall 0.6932 F1 0.7558\n",
      "val Loss: 0.8714 Acc: 0.4372 Percision: 0.6757 Recall 0.4380 F1 0.5137\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 0.3660 Acc: 0.6204 Percision: 0.8623 Recall 0.6925 F1 0.7546\n",
      "val Loss: 0.8661 Acc: 0.4372 Percision: 0.6706 Recall 0.4401 F1 0.5141\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 0.3617 Acc: 0.6226 Percision: 0.8369 Recall 0.6933 F1 0.7520\n",
      "val Loss: 0.8826 Acc: 0.4381 Percision: 0.7280 Recall 0.4506 F1 0.5352\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.3530 Acc: 0.6334 Percision: 0.8603 Recall 0.7112 F1 0.7741\n",
      "val Loss: 0.9130 Acc: 0.4391 Percision: 0.7244 Recall 0.4496 F1 0.5358\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.3507 Acc: 0.6341 Percision: 0.8620 Recall 0.7109 F1 0.7719\n",
      "val Loss: 0.8919 Acc: 0.4433 Percision: 0.6931 Recall 0.4666 F1 0.5427\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.3429 Acc: 0.6403 Percision: 0.8644 Recall 0.7160 F1 0.7769\n",
      "val Loss: 0.9606 Acc: 0.4381 Percision: 0.6901 Recall 0.4608 F1 0.5358\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.3426 Acc: 0.6453 Percision: 0.8507 Recall 0.7162 F1 0.7735\n",
      "val Loss: 0.9048 Acc: 0.4436 Percision: 0.6863 Recall 0.4467 F1 0.5218\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.3369 Acc: 0.6469 Percision: 0.8606 Recall 0.7279 F1 0.7848\n",
      "val Loss: 1.0182 Acc: 0.4407 Percision: 0.7155 Recall 0.4522 F1 0.5311\n",
      "\n",
      "Training complete in 136m 58s\n"
     ]
    }
   ],
   "source": [
    "#lr clcying\n",
    "max_cycle = 3\n",
    "for cycle in range(max_cycle):\n",
    "    # Observe that all parameters are being optimized\n",
    "    optimizer_ft = optim.Adam(model_ft.parameters(), lr=base_lr)\n",
    "\n",
    "    # Decay LR by a factor of 0.1 every 10 epochs\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.1)\n",
    "    model_ft, steps = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                           num_epochs=20, init_steps=steps)\n",
    "\n",
    "    #save intermediate model\n",
    "    torch.save(model_ft.state_dict(), 'M17_20181106_cycle_stage_{}.model'.format(cycle))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
